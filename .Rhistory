)
return(res)
})
})
train.test.folds
ind
dat
fold.train[ind]
dat <- fold.train[ind]
train.test.folds <- lapply(c(1:rep), function(split){
ind <- names(fold.train) %>% str_detect(paste("Rep",split, sep =""))
dat <- fold.train[ind]
res <- lapply(c(1:k), function(fold){
list(x.test = x[-dat[[fold]],],
x.train = x[dat[[fold]],],
y.test = y[-dat[[fold]]],
y.train = y[dat[[fold]]]
)
return(res)
})
})
train.test.folds
c(1:rep)
x
x[-dat[[1]],]
train.test.folds <- lapply(c(1:rep), function(split){
ind <- names(fold.train) %>% str_detect(paste("Rep",split, sep =""))
dat <- fold.train[ind]
res <- lapply(c(1:k), function(fold){
list(x.test = x[-dat[[fold]],],
x.train = x[dat[[fold]],],
y.test = y[-dat[[fold]]],
y.train = y[dat[[fold]]]
)
})
return(res)
})
train.test.folds
reps <- paste0("Rep", 1:rep)
folds <- paste0("Fold", 1:k)
train.test.folds <- setNames(lapply(train.test.folds, setNames, folds), reps)
set.seed(849)
models.lasso <-lapply(c(1:rep), function(split){
# select Data from 1 repeat
dat <- train.test.folds[[paste("Rep",split, sep ="")]]
message(paste("Starting calculation of Rep", split,"... of", rep))
# apply model to all folds of that 1 repeat and test against the remaining fold not used for training
res <- pblapply(c(1:k), function(fold){
calc.model.metrics.2(dat[[fold]]$x.train, y = dat[[fold]]$y.train, train.method = "glmnet",
tuneGrid = expand.grid(alpha = 1, lambda = seq(0.01,0.2,by = 0.01)))
})
return(res)
})
models.lasso
models.test <- models.lasso
# set names of list elements
models.test <- setNames(lapply(models.test, setNames, folds), reps)
df.train <- lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'train.metrics')) %>%
summarize(mean = mean(ROC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
df.train <-
df.train
df.train
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'train.metrics')) %>%
summarize(mean = mean(ROC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
df.train <- lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'train.metrics')) %>%
summarize(mean = mean(ROC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
df.train
df.test <- lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'test.metrics')) %>%
summarize(mean = mean(AUC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
df.test
library(missForest)
library(tidyverse)
library(devtools)
library(caret)
library(doParallel)
library(pROC)
library(pbapply)
# source R functions
source_url("https://raw.githubusercontent.com/MBender1992/base_scripts/Marc/R_functions.R")
test_class_cv_model$results$ROC %>% max()
pred <- predict(test_class_cv_model, x.test, type = "prob")
obs <- y.test
roc_obj <- roc(obs, pred$ja)
auc(roc_obj)
pred <- predict.train(test_class_cv_model, x.test, type = "prob")
obs <- y.test
roc_obj <- roc(obs, pred$ja)
auc(roc_obj)
pred <- predict(test_class_cv_model, type = "prob")
obs <- y.train
roc_obj <- roc(obs, pred$ja)
auc(roc_obj)
pred <- predict.train(test_class_cv_model, type = "prob")
obs <- y.train
roc_obj <- roc(obs, pred$ja)
auc(roc_obj)
x <- model.matrix.subset("baseline", data = dat_log)
# activate parallel computing
cl <- makeCluster(detectCores(), type='PSOCK')
registerDoParallel(cl)
# generate 5 folds for outer loop
rep <- 10
k <- 10
set.seed(12)
fold.train <- createMultiFolds(y, k = k, times = rep) # ensure that at least 10 samples are in each fold
# split data based on these folds (Fold1 means that Fold1 is used for testing)
train.test.folds <- lapply(c(1:rep), function(split){
# select only folds containing the specified repeat in each iteration
if(split == 10){
ind <- names(fold.train) %>% str_detect("Rep10")
dat <- fold.train[ind]
} else {
ind <- names(fold.train) %>% str_detect(paste("Rep0",split, sep =""))
dat <- fold.train[ind]
}
# split data into training and test set with each fold being the test set once
res <- lapply(c(1:k), function(fold){
list(x.test = x[-dat[[fold]],],
x.train = x[dat[[fold]],],
y.test = y[-dat[[fold]]],
y.train = y[dat[[fold]]]
)
})
return(res)
})
# define name of the list elements
reps <- paste0("Rep", 1:rep)
folds <- paste0("Fold", 1:k)
train.test.folds <- setNames(lapply(train.test.folds, setNames, folds), reps)
set.seed(849)
models.lasso.baseline  <-lapply(c(1:rep), function(split){
# select Data from 1 repeat
dat <- train.test.folds[[paste("Rep",split, sep ="")]]
# print message to follow progress
message(paste("Starting calculation of Rep", split,"... of", rep))
# apply model to all folds of that 1 repeat and test against the remaining fold not used for training
res <- pblapply(c(1:k), function(fold){
calc.model.metrics.2(x.train = dat[[fold]]$x.train, y.train = dat[[fold]]$y.train, x.test =dat[[fold]]$x.test,
y.test = dat[[fold]]$y.test, train.method = "glmnet",
tuneGrid = expand.grid(alpha = 1, lambda = seq(0.01,0.2,by = 0.01)))
})
return(res)
})
# set names of list elements
models.lasso.baseline <- setNames(lapply(models.lasso.baseline, setNames, folds), reps)
# extract train metrics from list and convert to data.frame
df.train <- lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.lasso.baseline[[split]], '[', 'train.metrics')) %>%
summarize(mean = mean(ROC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
#
df.test <- lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.lasso.baseline[[split]], '[', 'test.metrics')) %>%
summarize(mean = mean(AUC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
# extract important coefficients
extract.coefs <-lapply(1:10, function(x){
tmp <- sapply(sapply(models.lasso.baseline[[1]], '[', 'coefficients'), '[', 'coefs') %>% unlist()
data.frame(coef = tmp)
}) %>% do.call(rbind,.) %>% table()
# calculate percentages
feat.freq <- data.frame(sort(extract.coefs/100)) %>%
setNames(c("coef", "freq"))
# plot important features
ggplot(data = feat.freq, aes(coef, freq, fill = ifelse(freq > 0.5, "red", "blue"))) +
geom_bar(stat = "identity",  color = "black") +
coord_flip() +
xlab("") +
ylab("fraction of cv-models using this feature (relative feature importance)") +
theme_bw() +
scale_y_continuous(limits = c(0,1), breaks = seq(0,1,0.2), expand = c(0,0), labels = scales::percent_format()) +
geom_hline(yintercept = 0.5, lty = 2, color = "red") +
scale_fill_manual(labels = c("< 50 %", "> 50 %"), values = c("gray95", "lightblue")) +
labs(fill = "frequency")
library(missForest)
library(tidyverse)
library(devtools)
library(caret)
library(doParallel)
library(pROC)
library(pbapply)
calc.model.metrics.2 <- function(x.train, y.train, x.test, y.test, train.method = "glmnet", cv.method = "repeatedcv", number = 10, repeats = 5, metric = "ROC", tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred))
)
return(res)
}
#
model.matrix.subset <- function(model, data){
if(model == "complete"){
mm <- model.matrix(Responder~., data = data)[,-1]
} else if(model == "miRNA"){
mm <- model.matrix(Responder~., data = select(data, c(contains("mir"),Responder)))[,-1]
} else if(model == "baseline"){
mm <- model.matrix(Responder~., data = select(data, contains(c("Eosinophile","LDH","S100","CRP")),Responder))[,-1]
} else if(model == "signif"){
mm <- model.matrix(Responder~., data = select(data, contains(readRDS("significant_features.rds")),Responder))[,-1]
}
#
model.matrix.subset <- function(model, data){
if(model == "complete"){
mm <- model.matrix(Responder~., data = data)[,-1]
} else if(model == "miRNA"){
mm <- model.matrix(Responder~., data = select(data, c(contains("mir"),Responder)))[,-1]
} else if(model == "baseline"){
mm <- model.matrix(Responder~., data = select(data, contains(c("Eosinophile","LDH","S100","CRP")),Responder))[,-1]
} else if(model == "signif"){
mm <- model.matrix(Responder~., data = select(data, contains(readRDS("significant_features.rds")),Responder))[,-1]
} else {
stop("Please specify 1 of the following 4 options:
1. 'baseline' for a base model using conventional serum markers (LDH, CRP, S100, Eosinophile)
2. 'miRNA' for a model using only miRNAs (reduced by lasso to informative features)
3. 'signif' for a model with significantly different features between responders and non-responders
4. 'complete' for a model with all predictors (reduced by lasso)")
}
return(mm)
}
#
model.matrix.subset <- function(model, data){
if(model == "complete"){
mm <- model.matrix(Responder~., data = data)[,-1]
} else if(model == "miRNA"){
mm <- model.matrix(Responder~., data = select(data, c(contains("mir"),Responder)))[,-1]
} else if(model == "baseline"){
mm <- model.matrix(Responder~., data = select(data, contains(c("Eosinophile","LDH","S100","CRP")),Responder))[,-1]
} else if(model == "signif"){
mm <- model.matrix(Responder~., data = select(data, contains(readRDS("significant_features.rds")),Responder))[,-1]
} else {
stop("Please specify 1 of the following 4 options:
1. 'baseline' for a base model using conventional serum markers (LDH, CRP, S100, Eosinophile)
2. 'miRNA' for a model using only miRNAs (reduced by lasso to informative features)
3. 'signif' for a model with significantly different features between responders and non-responders
4. 'complete' for a model with all predictors (reduced by lasso)")
}
return(mm)
}
ds
}
#
model.matrix.subset <- function(model, data){
if(model == "complete"){
mm <- model.matrix(Responder~., data = data)[,-1]
} else if(model == "miRNA"){
mm <- model.matrix(Responder~., data = select(data, c(contains("mir"),Responder)))[,-1]
} else if(model == "baseline"){
mm <- model.matrix(Responder~., data = select(data, contains(c("Eosinophile","LDH","S100","CRP")),Responder))[,-1]
} else if(model == "signif"){
mm <- model.matrix(Responder~., data = select(data, contains(readRDS("significant_features.rds")),Responder))[,-1]
} else {
stop("Please specify 1 of the following 4 options:
1. 'baseline' for a base model using conventional serum markers (LDH, CRP, S100, Eosinophile)
2. 'miRNA' for a model using only miRNAs (reduced by lasso to informative features)
3. 'signif' for a model with significantly different features between responders and non-responders
4. 'complete' for a model with all predictors (reduced by lasso)")
}
return(mm)
}
calc.model.metrics.2 <- function(x.train, y.train, x.test, y.test, train.method = "glmnet", cv.method = "repeatedcv", number = 10, repeats = 5, metric = "ROC", tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred))
)
return(res)
}
x <- model.matrix.subset("signif", data = dat_log)
# activate parallel computing
cl <- makeCluster(detectCores(), type='PSOCK')
registerDoParallel(cl)
# generate 5 folds for outer loop
rep <- 10
k <- 10
set.seed(12)
fold.train <- createMultiFolds(y, k = k, times = rep) # ensure that at least 10 samples are in each fold
# split data based on these folds (Fold1 means that Fold1 is used for testing)
train.test.folds <- lapply(c(1:rep), function(split){
# select only folds containing the specified repeat in each iteration
if(split == 10){
ind <- names(fold.train) %>% str_detect("Rep10")
dat <- fold.train[ind]
} else {
ind <- names(fold.train) %>% str_detect(paste("Rep0",split, sep =""))
dat <- fold.train[ind]
}
# split data into training and test set with each fold being the test set once
res <- lapply(c(1:k), function(fold){
list(x.test = x[-dat[[fold]],],
x.train = x[dat[[fold]],],
y.test = y[-dat[[fold]]],
y.train = y[dat[[fold]]]
)
})
return(res)
})
# define name of the list elements
reps <- paste0("Rep", 1:rep)
folds <- paste0("Fold", 1:k)
train.test.folds <- setNames(lapply(train.test.folds, setNames, folds), reps)
set.seed(849)
models.lasso.signif <-lapply(c(1:rep), function(split){
# select Data from 1 repeat
dat <- train.test.folds[[paste("Rep",split, sep ="")]]
# print message to follow progress
message(paste("Starting calculation of Rep", split,"... of", rep))
# apply model to all folds of that 1 repeat and test against the remaining fold not used for training
res <- pblapply(c(1:k), function(fold){
calc.model.metrics.2(x.train = dat[[fold]]$x.train, y.train = dat[[fold]]$y.train, x.test =dat[[fold]]$x.test,
y.test = dat[[fold]]$y.test, train.method = "glmnet",
tuneGrid = expand.grid(alpha = 1, lambda = seq(0.01,0.2,by = 0.01)))
})
return(res)
})
# set names of list elements
models.lasso.signif <- setNames(lapply(models.lasso.signif, setNames, folds), reps)
# extract train metrics from list and convert to data.frame
df.train <- lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.lasso.signif[[split]], '[', 'train.metrics')) %>%
summarize(mean = mean(ROC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
#
df.test <- lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.lasso.signif[[split]], '[', 'test.metrics')) %>%
summarize(mean = mean(AUC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
# extract important coefficients
extract.coefs <-lapply(1:10, function(x){
tmp <- sapply(sapply(models.lasso.signif[[1]], '[', 'coefficients'), '[', 'coefs') %>% unlist()
data.frame(coef = tmp)
}) %>% do.call(rbind,.) %>% table()
# calculate percentages
feat.freq <- data.frame(sort(extract.coefs/100)) %>%
setNames(c("coef", "freq"))
# plot important features
ggplot(data = feat.freq, aes(coef, freq, fill = ifelse(freq > 0.5, "red", "blue"))) +
geom_bar(stat = "identity",  color = "black") +
coord_flip() +
xlab("") +
ylab("fraction of cv-models using this feature (relative feature importance)") +
theme_bw() +
scale_y_continuous(limits = c(0,1), breaks = seq(0,1,0.2), expand = c(0,0), labels = scales::percent_format()) +
geom_hline(yintercept = 0.5, lty = 2, color = "red") +
scale_fill_manual(labels = c("< 50 %", "> 50 %"), values = c("gray95", "lightblue")) +
labs(fill = "frequency")
# load packages
library(missForest)
library(tidyverse)
library(devtools)
library(caret)
library(doParallel)
library(pROC)
library(pbapply)
# source R functions
source_url("https://raw.githubusercontent.com/MBender1992/base_scripts/Marc/R_functions.R")
calc.model.metrics.2 <- function(x.train, y.train, x.test, y.test, train.method = "glmnet", cv.method = "repeatedcv", number = 10, repeats = 5, metric = "ROC", tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred))
)
return(res)
}
#
model.matrix.subset <- function(model, data){
if(model == "complete"){
mm <- model.matrix(Responder~., data = data)[,-1]
} else if(model == "miRNA"){
mm <- model.matrix(Responder~., data = select(data, c(contains("mir"),Responder)))[,-1]
} else if(model == "baseline"){
mm <- model.matrix(Responder~., data = select(data, contains(c("Eosinophile","LDH","S100","CRP")),Responder))[,-1]
} else if(model == "signif"){
mm <- model.matrix(Responder~., data = select(data, contains(readRDS("significant_features.rds")),Responder))[,-1]
} else {
stop("Please specify 1 of the following 4 options:
1. 'baseline' for a base model using conventional serum markers (LDH, CRP, S100, Eosinophile)
2. 'miRNA' for a model using only miRNAs (reduced by lasso to informative features)
3. 'signif' for a model with significantly different features between responders and non-responders
4. 'complete' for a model with all predictors (reduced by lasso)")
}
return(mm)
}
x <- model.matrix.subset("signif", data = dat_log)
# activate parallel computing
cl <- makeCluster(detectCores(), type='PSOCK')
registerDoParallel(cl)
# generate 5 folds for outer loop
rep <- 10
k <- 10
set.seed(12)
fold.train <- createMultiFolds(y, k = k, times = rep) # ensure that at least 10 samples are in each fold
# split data based on these folds (Fold1 means that Fold1 is used for testing)
train.test.folds <- lapply(c(1:rep), function(split){
# select only folds containing the specified repeat in each iteration
if(split == 10){
ind <- names(fold.train) %>% str_detect("Rep10")
dat <- fold.train[ind]
} else {
ind <- names(fold.train) %>% str_detect(paste("Rep0",split, sep =""))
dat <- fold.train[ind]
}
# split data into training and test set with each fold being the test set once
res <- lapply(c(1:k), function(fold){
list(x.test = x[-dat[[fold]],],
x.train = x[dat[[fold]],],
y.test = y[-dat[[fold]]],
y.train = y[dat[[fold]]]
)
})
return(res)
})
# define name of the list elements
reps <- paste0("Rep", 1:rep)
folds <- paste0("Fold", 1:k)
train.test.folds <- setNames(lapply(train.test.folds, setNames, folds), reps)
set.seed(849)
models.lasso.signif <-lapply(c(1:rep), function(split){
# select Data from 1 repeat
dat <- train.test.folds[[paste("Rep",split, sep ="")]]
# print message to follow progress
message(paste("Starting calculation of Rep", split,"... of", rep))
# apply model to all folds of that 1 repeat and test against the remaining fold not used for training
res <- pblapply(c(1:k), function(fold){
calc.model.metrics.2(x.train = dat[[fold]]$x.train, y.train = dat[[fold]]$y.train, x.test =dat[[fold]]$x.test,
y.test = dat[[fold]]$y.test, train.method = "glmnet",
tuneGrid = expand.grid(alpha = 1, lambda = seq(0.01,0.2,by = 0.01)))
})
return(res)
})
# set names of list elements
models.lasso.signif <- setNames(lapply(models.lasso.signif, setNames, folds), reps)
# extract train metrics from list and convert to data.frame
df.train <- lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.lasso.signif[[split]], '[', 'train.metrics')) %>%
summarize(mean = mean(ROC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
#
df.test <- lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.lasso.signif[[split]], '[', 'test.metrics')) %>%
summarize(mean = mean(AUC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
# extract important coefficients
extract.coefs <-lapply(1:10, function(x){
tmp <- sapply(sapply(models.lasso.signif[[1]], '[', 'coefficients'), '[', 'coefs') %>% unlist()
data.frame(coef = tmp)
}) %>% do.call(rbind,.) %>% table()
# calculate percentages
feat.freq <- data.frame(sort(extract.coefs/100)) %>%
setNames(c("coef", "freq"))
# plot important features
ggplot(data = feat.freq, aes(coef, freq, fill = ifelse(freq > 0.5, "red", "blue"))) +
geom_bar(stat = "identity",  color = "black") +
coord_flip() +
xlab("") +
ylab("fraction of cv-models using this feature (relative feature importance)") +
theme_bw() +
scale_y_continuous(limits = c(0,1), breaks = seq(0,1,0.2), expand = c(0,0), labels = scales::percent_format()) +
geom_hline(yintercept = 0.5, lty = 2, color = "red") +
scale_fill_manual(labels = c("< 50 %", "> 50 %"), values = c("gray95", "lightblue")) +
labs(fill = "frequency")
