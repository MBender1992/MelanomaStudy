Spec = opt$Spec),
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred)),
hyperParams = md$bestTune
)
return(res)
}
calc.model.metrics2 <- function(x,y, method = "repeatedcv", number = 10, repeats = 5, metric = "ROC", tuneGrid){
# define training and test set
ind.train <- createDataPartition(y, p = 0.7, list = FALSE)
# split predictors into training and test set
x.train  <- x[ind.train, ]
x.test <- x[-ind.train, ]
# split response into training and test set
y.train <- y[ind.train]
y.test <- y[-ind.train]
calc.model.metrics(x.train=x.train, y.train = y.train, train.method = "glmnet", number = number, repeats = repeats, metric = metric, tuneGrid = tuneGrid)
}
set.seed(12)
test <- calc.model.metrics2(x = x, y = y, train.method = "glmnet", tuneGrid = expand.grid(alpha = seq(0,1,0.1), lambda = seq(0.01,0.2,by = 0.01)))
set.seed(12)
test <- calc.model.metrics2(x = x, y = y,  tuneGrid = expand.grid(alpha = seq(0,1,0.1), lambda = seq(0.01,0.2,by = 0.01)))
calc.model.metrics <- function(x.train, y.train,cv.method, train.method, number, repeats,metric, tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
set.seed(849)
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = data.frame(AUC = opt$ROC,
Sens = opt$Sens,
Spec = opt$Spec),
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred)),
hyperParams = md$bestTune
)
return(res)
}
calc.model.metrics2 <- function(x,y, cv.method = "repeatedcv", number = 10, repeats = 5, metric = "ROC", tuneGrid){
# define training and test set
ind.train <- createDataPartition(y, p = 0.7, list = FALSE)
# split predictors into training and test set
x.train  <- x[ind.train, ]
x.test <- x[-ind.train, ]
# split response into training and test set
y.train <- y[ind.train]
y.test <- y[-ind.train]
calc.model.metrics(x.train=x.train, y.train = y.train,method = cv.method, train.method = "glmnet", number = number, repeats = repeats, metric = metric, tuneGrid = tuneGrid)
}
set.seed(12)
test <- calc.model.metrics2(x = x, y = y,  tuneGrid = expand.grid(alpha = seq(0,1,0.1), lambda = seq(0.01,0.2,by = 0.01)))
calc.model.metrics <- function(x.train, y.train,cv.method, train.method, number, repeats,metric, tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
set.seed(849)
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = data.frame(AUC = opt$ROC,
Sens = opt$Sens,
Spec = opt$Spec),
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred)),
hyperParams = md$bestTune
)
return(res)
}
test <- calc.model.metrics(x,y, cv.method = "repeatedcv", train.method = "glmnet", number = 10, repeats = 1, metric = "ROC", , tuneGrid = expand.grid(alpha = seq(0,1,0.1), lambda = seq(0.01,0.2,by = 0.01)))
test <- calc.model.metrics(x,y, cv.method = "repeatedcv", train.method = "glmnet", number = 10, repeats = 1, metric = "ROC", tuneGrid = expand.grid(alpha = seq(0,1,0.1), lambda = seq(0.01,0.2,by = 0.01)))
test
opt
calc.model.metrics <- function(x.train, y.train,cv.method, train.method, number, repeats,metric, tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
set.seed(849)
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt,
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred)),
hyperParams = md$bestTune
)
return(res)
}
test <- calc.model.metrics(x,y, cv.method = "repeatedcv", train.method = "glmnet", number = 10, repeats = 1, metric = "ROC", tuneGrid = expand.grid(alpha = seq(0,1,0.1), lambda = seq(0.01,0.2,by = 0.01)))
test
opt
opt
opt[which(max(ROC))]
opt[which(max(opt$ROC))]
max(opt$ROC)
opt[which(opt == max(opt$ROC))]
opt == max(opt$ROC)
opt$ROC == max(opt$ROC))
which(opt$ROC == max(opt$ROC))
opt[which(opt$ROC == max(opt$ROC))]
opt[which(opt$ROC == max(opt$ROC)),]
res$train.metrics
opt[which(opt$ROC == max(opt$ROC)),]
md$bestTune
calc.model.metrics <- function(x.train, y.train,cv.method, train.method, number, repeats,metric, tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
set.seed(849)
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred)),
hyperParams = md$bestTune
)
return(res)
}
test <- calc.model.metrics(x,y, cv.method = "repeatedcv", train.method = "glmnet", number = 10, repeats = 1, metric = "ROC", tuneGrid = expand.grid(alpha = seq(0,1,0.1), lambda = seq(0.01,0.2,by = 0.01)))
test
(filter(opt, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD))
select(opt, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD)
select(opt, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD)
select(opt, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD))
select(opt, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD))
select(opt, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD))
identical(select(opt, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD)), md$bestTune)
!identical(select(opt, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD)), md$bestTune)
calc.model.metrics <- function(x.train, y.train,cv.method, train.method, number, repeats,metric, tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
set.seed(849)
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred)),
hyperParams = md$bestTune
)
ifelse(!identical(select(opt, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD)), md$bestTune),
stop("The tuning parameters used to extract AUC do not match the tuning parameters returned by 'best tune'"),
return(res))
}
calc.model.metrics <- function(x.train, y.train,cv.method, train.method, number, repeats,metric, tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
set.seed(849)
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred))
)
ifelse(!identical(select(opt, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD)), md$bestTune),
stop("The tuning parameters used to extract AUC do not match the tuning parameters returned by 'best tune'"),
return(res))
}
test <- calc.model.metrics(x,y, cv.method = "repeatedcv", train.method = "glmnet", number = 10, repeats = 1, metric = "ROC", tuneGrid = expand.grid(alpha = seq(0,1,0.1), lambda = seq(0.01,0.2,by = 0.01)))
x
y
set.seed(849)
md <- train(x, y, method = "glmnet",preProcess = c("center","scale"),
trControl = cctrl1,metric = "ROC",tuneGrid =  expand.grid(alpha = seq(0,1,0.1), lambda = seq(0.01,0.2,by = 0.01)))
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred))
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred))
)
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred))
)
res
select(train.metrics, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD))
select(res$train.metrics, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD))
md$bestTune
identical(select(res$train.metrics, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD)), md$bestTune
identical(select(res$train.metrics, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD)), md$bestTune)
identical(select(res$train.metrics, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD)), md$bestTune)
calc.model.metrics <- function(x.train, y.train,cv.method, train.method, number, repeats,metric, tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
set.seed(849)
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred))
)
ifelse(!identical(select(res$train.metrics, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD)), md$bestTune),
stop("The tuning parameters used to extract AUC do not match the tuning parameters returned by 'best tune'"),
return(res))
}
test <- calc.model.metrics(x,y, cv.method = "repeatedcv", train.method = "glmnet", number = 10, repeats = 1, metric = "ROC", tuneGrid = expand.grid(alpha = seq(0,1,0.1), lambda = seq(0.01,0.2,by = 0.01)))
test
calc.model.metrics2 <- function(x,y, cv.method = "repeatedcv", number = 10, repeats = 5, metric = "ROC", tuneGrid){
# define training and test set
ind.train <- createDataPartition(y, p = 0.7, list = FALSE)
# split predictors into training and test set
x.train  <- x[ind.train, ]
x.test <- x[-ind.train, ]
# split response into training and test set
y.train <- y[ind.train]
y.test <- y[-ind.train]
calc.model.metrics(x.train=x.train, y.train = y.train,cv.method = cv.method, train.method = "glmnet", number = number, repeats = repeats, metric = metric, tuneGrid = tuneGrid)
}
set.seed(12)
test <- calc.model.metrics2(x = x, y = y,  tuneGrid = expand.grid(alpha = seq(0,1,0.1), lambda = seq(0.01,0.2,by = 0.01)))
ind.train <- createDataPartition(y, p = 0.75, list = FALSE)
set.seed(12)
ind.train <- createDataPartition(y, p = 0.75, list = FALSE)
ind.train
set.seed(12)
ind.train <- createDataPartition(y, p = 0.75, list = T, times = 4)
ind.train
createDataPartition(y, p = 0.75, list = F, times = 4)
x.train1 <-
?createDataPartition
x.train1 <-
?createFolds
?createFolds
createFolds(y, k = 4)
createFolds(y, k = 3)
createFolds(y, k = 5)
createFolds(y, k = 4)
createFolds(y, k = 5)
fold.train$Fold1
fold.train <- createFolds(y, k = 5) # ensure that at least 10 samples are in each fold
fold.train$Fold1
x.test1 <- x[fold.train$Fold1]
x.test1
x[fold.train$Fold1,]
x.train1 <- x[-fold.train$Fold1,]
x.train1
fold.train[[1]]
lapply(1:5, function(split){
x.test1 <- x[fold.train[[split]],]
x.train1 <- x[-fold.train[[split]],]
y.test1 <- y[fold.train[[split]],]
y.train1 <- y[fold.train[[split]],]
})
lapply(c(1:5), function(split){
x.test1 <- x[fold.train[[split]],]
x.train1 <- x[-fold.train[[split]],]
y.test1 <- y[fold.train[[split]],]
y.train1 <- y[fold.train[[split]],]
})
function(split){
x.test1 <- x[fold.train[[split]],]
x.train1 <- x[-fold.train[[split]],]
y.test1 <- y[fold.train[[split]],]
y.train1 <- y[fold.train[[split]],]
})
test <- function(split){
x.test1 <- x[fold.train[[split]],]
x.train1 <- x[-fold.train[[split]],]
y.test1 <- y[fold.train[[split]],]
y.train1 <- y[fold.train[[split]],]
}
test(1)
return(x.test1)
test <- function(split){
x.test1 <- x[fold.train[[split]],]
x.train1 <- x[-fold.train[[split]],]
y.test1 <- y[fold.train[[split]],]
y.train1 <- y[fold.train[[split]],]
return(x.test1)
}
test(1)
x.test1 <- x[fold.train[[1]],]
x.train1 <- x[-fold.train[[1]],]
y
fold.train[[1]]
lapply(c(1:5), function(split){
x.test1 <- x[fold.train[[split]],]
x.train1 <- x[-fold.train[[split]],]
y.test1 <- y[fold.train[[split]]]
y.train1 <- y[fold.train[[split]]]
})
list(x.test = x[fold.train[[1]],],
x.train = x[-fold.train[[1]],],
y.test = y[fold.train[[1]]],
y.train = y[fold.train[[1]]]
)
list(x.test = x[fold.train[[1]],],
x.train = x[-fold.train[[1]],],
y.test = y[fold.train[[1]]],
y.train = y[-fold.train[[1]]]
)
test <- lapply(c(1:5), function(split){
list(x.test = x[fold.train[[split]],],
x.train = x[-fold.train[[split]],],
y.test = y[fold.train[[split]]],
y.train = y[fold.train[[split]]]
)
})
test
names(test)
names(test) <- c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")
test
fold.train
train.test.folds <- lapply(c(1:5), function(split){
list(x.test = x[fold.train[[split]],],
x.train = x[-fold.train[[split]],],
y.test = y[fold.train[[split]]],
y.train = y[fold.train[[split]]]
)
})
# set names
names(train.test.folds) <- c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")
train.test.folds
train.test.folds$Fold1
train.test.folds[[1]]
train.test.folds$Fold1$y.train
train.test.folds
fold.train <- createFolds(y, k = 5) # ensure that at least 10 samples are in each fold
# split data based on these folds (Fold1 means that Fold1 is used for testing)
train.test.folds <- lapply(c(1:5), function(split){
list(x.test = x[fold.train[[split]],],
x.train = x[-fold.train[[split]],],
y.test = y[fold.train[[split]]],
y.train = y[-fold.train[[split]]]
)
})
# set names
names(train.test.folds) <- c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")
train.test.folds$Fold1$y.train
train.test.folds[[1]]$y.train
models <- calc.model.metrics(x = train.test.folds$Fold1$x.test, y = train.test.folds$Fold1$y.train, train.method = "glmnet", tuneGrid = expand.grid(alpha = seq(0,1,0.1), lambda = seq(0.01,0.2,by = 0.01)))
calc.model.metrics <- function(x.train, y.train,cv.method = "repeatedcv", number = 10, repeats = 5, metric = "ROC", tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
set.seed(849)
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred))
)
ifelse(!identical(select(res$train.metrics, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD)), md$bestTune),
stop("The tuning parameters used to extract AUC do not match the tuning parameters returned by 'best tune'"),
return(res))
}
models <- calc.model.metrics(x = train.test.folds$Fold1$x.test, y = train.test.folds$Fold1$y.train, train.method = "glmnet", tuneGrid = expand.grid(alpha = seq(0,1,0.1), lambda = seq(0.01,0.2,by = 0.01)))
calc.model.metrics <- function(x.train, y.train,train.method = "glmnet", cv.method = "repeatedcv", number = 10, repeats = 5, metric = "ROC", tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
set.seed(849)
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred))
)
ifelse(!identical(select(res$train.metrics, -c(ROC, Sens, Spec, ROCSD, SensSD, SpecSD)), md$bestTune),
stop("The tuning parameters used to extract AUC do not match the tuning parameters returned by 'best tune'"),
return(res))
}
models <- calc.model.metrics(x = train.test.folds$Fold1$x.test, y = train.test.folds$Fold1$y.train, train.method = "glmnet", tuneGrid = expand.grid(alpha = seq(0,1,0.1), lambda = seq(0.01,0.2,by = 0.01)))
train.test.folds$Fold1$x.test
train.test.folds$Fold1$y.train
models <- calc.model.metrics(x = train.test.folds$Fold1$x.train, y = train.test.folds$Fold1$y.train, train.method = "glmnet", tuneGrid = expand.grid(alpha = seq(0,1,0.1), lambda = seq(0.01,0.2,by = 0.01)))
models
set.seed(12)
fold.train <- createFolds(y, k = 5) # ensure that at least 10 samples are in each fold
# split data based on these folds (Fold1 means that Fold1 is used for testing)
train.test.folds <- lapply(c(1:5), function(split){
list(x.test = x[fold.train[[split]],],
x.train = x[-fold.train[[split]],],
y.test = y[fold.train[[split]]],
y.train = y[-fold.train[[split]]]
)
})
# set names
names(train.test.folds) <- c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")
length(train.test.folds)
models
models <- lapply(1:length(train.test.folds), function(fold){
calc.model.metrics(train.test.folds[[fold]]$x.train, y = train.test.folds[[fold]]$y.train, train.method = "glmnet",  tuneGrid = expand.grid(alpha = seq(0,1,0.1), lambda = seq(0.01,0.2,by = 0.01)))
})
models
