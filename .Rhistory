models.lasso.complete$Rep10
models.lasso.complete[[1]]
sapply(models.lasso.complete[[1]], "[", "predictions")
sapply(models.lasso.complete[[1]], "[", "predictions$pred.ja")
test <- sapply(models.lasso.complete[[1]], "[", "predictions")
test
sapply(test,"$", "pred.ja")
test
test[[1]]
test[[1]]$pred.ja
lapply(1:10, function(x){
test[[x]]$pred.ja
}
lapply(1:10, function(x){
test[[x]]$pred.ja
})
lapply(1:10, function(x){
test[[x]]$pred.ja
})
lapply(1:10, function(d){
sapply(models.lasso.complete[[d]], "[", "predictions")
)})
lapply(1:10, function(d){
sapply(models.lasso.complete[[d]], "[", "predictions")
})
test2 <- lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$pred.ja
})
})
test2
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$pred.ja
})
})
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
out <- cvAUC(predictions, labels)
librar(cvAUC)
library(cvAUC)
out <- cvAUC(predictions, labels)
setwd("C:/Marc/Arbeit/R/base_scripts")
#loading custom functions
source("R_functions.R")
library(caret)
library(AppliedPredictiveModeling)
library(car)
library(pROC)
library(doParallel)
library(gbm)
library(tidyverse)
library(corrplot)
library(PerformanceAnalytics)
library(caretEnsemble)
library(kernlab)
library(xgboost)
library(xlsx)
library(RANN)
library(mice)
library(missForest)
library(cvAUC)
library(devtools)
# set working directory
setwd("C:/Marc/Arbeit/R/Projekte/Doktorarbeiten_Melanom_Liquid_Biopsies/Doktorarbeiten_Melanom")
###################################################################################
#                                                                                 #
#                             1. raw Data processing                              #
#                                                                                 #
###################################################################################
# set working directory
setwd("Z:/Aktuell/Eigene Dateien/Eigene Dateien_Marc/R/Projekte/Doktorarbeiten_Melanom_Liquid_Biopsies/Daten")
#load data
dat_combined <- load_melanoma_data() %>% filter(!is.na(Responder)& miRExpAssess == 1)
#detect columns containing NAs
NAs <-sapply(dat_combined, function(df){
sum(is.na(df) ==TRUE)/length(df);
})
#index of columns which contain NAs
which(NAs != 0)
# impute missing values in numeric data (befallene Organe, age,S100) by random forest iteration
dat_imp <- dat_combined %>% select_if(is.numeric) %>% as.data.frame() %>% missForest()
## add NA in factors as factor level and indclude imputed values for NA in numerical data
dat_complete <- dat_combined %>%
# mutate(Hirnmetastase = addNA(Hirnmetastase),
#        Stadium = addNA(Stadium),
#        adjuvant_IFN = addNA(adjuvant_IFN)) %>%
mutate(age = dat_imp$ximp$age,
befallen_Organe = dat_imp$ximp$befallen_Organe,
S100 = dat_imp$ximp$S100, Responder = factor(Responder))
#summary statistics
summary(dat_complete)
# Test ob Hirnmetastase oder Stadium einen Einfluss auf Model Performance haben
# bei Trainingsset kein Einfluss, möglicherweise bei Testset?
# library(mice)
# names(dat_complete) <- str_replace_all(names(dat_complete),"-","_")
# dat_complete_2 <- dat_complete #%>% mutate(Stadium_IV = factor(ifelse(Stadium == "IV",1,0)),
#                                #           BM_yes = factor(ifelse(Hirnmetastase =="ja",1,0)))
# imp <- mice(dat_complete_2 , seed = 1233, method = "logreg")
# dat_complete_2 <- complete(imp) #%>% select(-c(Stadium,Hirnmetastase)) %>% droplevels()
# names(dat_complete_2) <- str_replace_all(names(dat_complete_2),"_","-")
#
# dat_complete <- dat_complete_2
dat_complete <- dat_complete %>% select(-c(Baseline, TRIM_PDL1_Expression, therapy_at_blood_draw, subtype, localization,miRExpAssess, nras,adjuvant_IFN, organsInvolved))
dat_complete$sex <- factor(dat_complete$sex)
dat_complete$brainMet <- factor(dat_complete$brainMet)
dat_complete$Stadium <- factor(dat_complete$Stadium)
dat_complete$prior_BRAF_therapy <- factor(dat_complete$prior_BRAF_therapy)
###################################################################################
#                                                                                 #
#                             2. Data transformation                              #
#                                                                                 #
###################################################################################
#exclude Responder from one-hot encoding
index_Responder <- grep("Responder", colnames(dat_complete))
# one hot encoding for categorical variables
dmy <- dummyVars(" ~ .", data = dat_complete[,-index_Responder])
dat_dummy <- data.frame(predict(dmy, newdata = dat_complete[,-index_Responder]))
#add Responder column
dat_dummy <- dat_dummy %>% mutate(Responder = dat_complete$Responder)
###################################################################################
#                                                                                 #
#                    3.Feature selection/inspection                               #
#                                                                                 #
###################################################################################
# use only significantly changed parameters in model (+ miR-137 as RFE suggested it to be a good feature, miR-514a
# lowly expressed, so maybe not?, miR-509-3p dazu? Feature miR-29c-3p zwar nicht signifikant aber geringer p-value und performt gut bei RFE
# allerdings hohe Korrelation mit miR-29b-3p)
# colums containg more than 5% NAs were omitted for the analysis other missing values were imputed by a random forest imputation
# miR-34a und miR-514 lieferten keine gute Performance
dat_signif <- dat_dummy %>% select(X.hsa.mir.137.,X.hsa.mir.29c.3p.,X.hsa.mir.29b.3p., BRAFpos, BRAFneg, Eosinophile, CRP, LDH, S100,Responder)
#log transform lab parameters as they seem to follow a log-normal distribution and remove highly correlated miR-29c
dat_model <- dat_signif %>%
select(-c(X.hsa.mir.29c.3p.)) %>%
mutate(Eosinophile = ifelse(is.infinite(log(Eosinophile )),0,log(Eosinophile )),
CRP = log(CRP),
X.hsa.mir.137. = log(X.hsa.mir.137.)) # training data performt besser ohne log miR-137, bei Test data auch so?
# die 3 transformierten Variablen zeigen durch Transformation eine deutlich bessere Approximation an die Normalverteilung
# Die Verteilung von miR-29b ist schon untransformatiert sehr nah an einer Normalverteilung
# Die Transformation von LDH und S100 ändert nicht viel an der "Normalität" der Verteilung
# -> Model einmal mit log-transformierten und einmal mit untransformierten LDH,S100, miR-29b gefittet
# -> besseres Ergebnis wenn LDH und S100 nicht transformiert werden
###################################################################################
#                                                                                 #
#                                    4. Modelling                                 #
#                                                                                 #
###################################################################################
# (so wie es später sein sollte, lediglich die "extraProb" Daten durch Predict ersetzen (siehe Diabetes Datenset))
# da durch extractProb in diesem Beispiel nur die Probs des final models extrahiert werden (das final model entspricht dem Algorithmus auf dem gesamten Trainingsset)
# parameter tuning findet durch cross validation statt aber anschließend wird das Model mit den besten Tuningoptionen  noch einmal auf das gesamte Trainingsset angewendet um die endgültigen
# Koeffizienten für die Features zu erhalten
# activate parallel computing
cl <- makeCluster(detectCores(), type='PSOCK')
registerDoParallel(cl)
# define model ensemble with svm, random forest, gradient boosted tree and gradient boosted linear model
# preprocess within model as values are back transformed afterwards, z-value scaling used
# to customize the model and change tuning parameters use tuneList and define tuneGrid or tuneLength
# define control for cross validation
set.seed(12)
trainctrl <- trainControl(classProbs= TRUE,
verboseIter = TRUE,
summaryFunction = twoClassSummary,
method = "repeatedcv",
number = 10,
repeats = 3,
returnResamp = "all", # if error: change back to final
savePredictions = "all",
returnData = TRUE,
allowParallel = TRUE)
#define algorithms and specify hyperparamters
algorithmList <- c("xgbTree","svmRadial","lda","glmnet") #svm durch andere Algorithmen ersetzen? lvq zb use more models?
TuneList <- list(rf=caretModelSpec(method="rf", tuneGrid=data.frame(.mtry=1:10)))
dat_model <- dat_model %>% filter(!is.na(BRAFpos))
# run model
set.seed(222)
model_list <- caretList(dat_model[,-9], dat_model[,9],
trControl = trainctrl,
methodList = algorithmList,
tuneList = TuneList,
continue_on_fail = FALSE,
preProcess = c("center","scale"))
#Results
# define different StackControl as the indexes will be wrong otherwise
set.seed(222)
stackControl <- trainControl(
method="repeatedcv",
number=10,
repeats=3,
savePredictions="final",
classProbs=TRUE,
summaryFunction=twoClassSummary)
#calculate stacked model with knn (ohne separaten Seed zu setzen und durch ausführen zusammen mit dem anderen Model kommt man auf 0.924 wie in dem Vortrag)
set.seed(56)
ensemble_3 <- caretStack(model_list,
method = "knn",
metric = "ROC",
tuneGrid = data.frame(k = seq(1, 15, 1)),
trControl = stackControl)
#results of ensemble 3
summary(ensemble_3)
print(ensemble_3)
model_list
split(model_list$svmRadial$pred$nein, f = model_list$svmRadial$pred$Resample)
class(predictions)
names(predictions)
predictions <- split(model_list$svmRadial$pred$nein, f = model_list$svmRadial$pred$Resample)
class(predictions)
names(predictions)
names(predictions) <- NULL
cvAUC(predictions, labels)
labels <- split(model_list$svmRadial$pred$obs, f = model_list$svmRadial$pred$Resample)
cvAUC(predictions, labels)
predictions
predictions
lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$pred.ja
})
})
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$pred.ja
})
})
lapply(rapply(predictions, enquote, how="unlist"), eval)
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
labels
cvAUC(predictions, labels)
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$pred.nein
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
cvAUC(predictions, labels)
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
1-test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
out <- cvAUC(predictions, labels)
out
models.lasso.complete
ci.complete
predictions
predictions
labels
split(model_list$svmRadial$pred$nein, f = model_list$svmRadial$pred$Resample)
split(model_list$svmRadial$pred$obs, f = model_list$svmRadial$pred$Resample)
unlist.model(models.lasso.complete, "AUC", "test.metrics")
unlist.model(models.lasso.complete, "AUC", "test.metrics") %>% mean()
models.lasso.complete
unlist.model(models.lasso.complete, "pred.ja", "predictions")
models.lasso.complete
unlist.model(model, "AUC", "test.metrics")
unlist.model(models.lasso.complete, "AUC", "test.metrics")
models.lasso.complete$Rep10
models.lasso.complete$Rep10$Fold10
models.lasso.complete$Rep10$Fold10$predictions
auc(models.lasso.complete$Rep10$Fold10$obs, models.lasso.complete$Rep10$Fold10$pred.ja)
models.lasso.complete$Rep10$Fold10$obs
models.lasso.complete$Rep10$Fold10
auc(models.lasso.complete$Rep10$Fold10$predictions$obs, models.lasso.complete$Rep10$Fold10$predictions$pred.ja)
cvAUC(models.lasso.complete$Rep10$Fold10$predictions$pred.ja, models.lasso.complete$Rep10$Fold10$predictions$obs)
cvAUC(1-models.lasso.complete$Rep10$Fold10$predictions$pred.ja, models.lasso.complete$Rep10$Fold10$predictions$obs)
cvAUC(predictions, labels)
unlist.model(models.lasso.complete)
unlist.model(models.lasso.complete, "AUC", "test.metrics")
out
data.frame(out$fold.AUC, unlist.model(models.lasso.complete, "AUC", "test.metrics"))
data.frame(cvAUC = out$fold.AUC, modelAUC = unlist.model(models.lasso.complete, "AUC", "test.metrics"))
models.lasso.complete$Rep10$Fold7
predictions
predictions[[97]]
labels[[97]]
models.lasso.complete$Rep10$Fold7
predictions[[97]]
labels[[97]]
models.lasso.complete$Rep10$Fold7$predictions
?cvAUC
cvAUC
predictions[[97]]
ROCR::prediction(predictions[[97]],labels[[97]])
pred <- ROCR::prediction(predictions[[97]],labels[[97]])
perf <- ROCR::performance(pred, "tpr", "fpr")
perf
ROCR::performance(pred, measure = "auc",
x.measure = "cutoff")
ROCR::performance(pred, measure = "auc",
x.measure = "cutoff")@y.values
as.numeric(ROCR::performance(pred, measure = "auc",
x.measure = "cutoff")@y.values)
?performance
?auc
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
1-test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
cvAUC(predictions, labels)
ci.relaxedLasso
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.baseline[[d]], "[", "predictions")
lapply(1:10, function(x){
1-test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.baseline[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
out <- cvAUC(predictions, labels)
cvAUC(predictions, labels)
ci.baseline
confInt(out$fold.AUC)
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.signif[[d]], "[", "predictions")
lapply(1:10, function(x){
1-test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.signif[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
out <- cvAUC(predictions, labels)
out
auc(predictions[[97]],labels[[97]])
auc(labels[[97]],predictions[[97]])
pred <- ROCR::prediction(predictions[[97]],labels[[97]])
as.numeric(ROCR::performance(pred, measure = "auc",
x.measure = "cutoff")@y.values)
pred <- ROCR::prediction(predictions[[97]],labels[[97]])
as.numeric(ROCR::performance(pred, measure = "auc",
x.measure = "cutoff")@y.values)
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
1-test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
out <- cvAUC(predictions, labels)
data.frame(cvAUC = out$fold.AUC, modelAUC = unlist.model(models.lasso.complete, "AUC", "test.metrics"))
pred <- ROCR::prediction(predictions[[97]],labels[[97]])
perf <- ROCR::performance(pred, "tpr", "fpr")
as.numeric(ROCR::performance(pred, measure = "auc",
x.measure = "cutoff")@y.values)
auc(labels[[97]],predictions[[97]])
predictions[[97]]
predictions[[97]]
labels[[97]]
predictions[[97]]
labels[[97]]
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
out <- cvAUC(predictions, labels)
unlist.model(models.lasso.complete, "AUC", "test.metrics")
data.frame(cvAUC = out$fold.AUC, modelAUC = unlist.model(models.lasso.complete, "AUC", "test.metrics"))
models.lasso.complete$Rep10$Fold7$predictions
pred <- ROCR::prediction(predictions[[97]],labels[[97]])
perf <- ROCR::performance(pred, "tpr", "fpr")
as.numeric(ROCR::performance(pred, measure = "auc",
x.measure = "cutoff")@y.values)
auc(labels[[97]],predictions[[97]])
data.frame(cvAUC = out$fold.AUC, modelAUC = unlist.model(models.lasso.complete, "AUC", "test.metrics"))
pred <- ROCR::prediction(predictions[[98]],labels[[98]])
perf <- ROCR::performance(pred, "tpr", "fpr")
as.numeric(ROCR::performance(pred, measure = "auc",
x.measure = "cutoff")@y.values)
auc(labels[[98]],predictions[[98]])
predictions[[98]]
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$pred.ja
})
})
predictions
predictions[[98]],labels[[98]]
predictions[[98]]
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
predictions[[98]]
labels[[98]]
pred <- ROCR::prediction(predictions[[98]],labels[[98]])
perf <- ROCR::performance(pred, "tpr", "fpr")
as.numeric(ROCR::performance(pred, measure = "auc",
x.measure = "cutoff")@y.values)
test <- ROCR::performance(pred, measure = "auc",
x.measure = "cutoff")
test@x.values
test@y.values
test@alpha.values
roc(labels[[98]],predictions[[98]])
auc(roc(labels[[98]],predictions[[98]]))
predictions[[98]]
auc(roc(labels[[98]],predictions[[98]], direction = ">"))
auc(roc(labels[[98]],predictions[[98]], direction = "<"))
auc(roc(labels[[98]],predictions[[98]], direction = "<", levels = c("nein", "ja")))
auc(roc(labels[[98]],predictions[[98]], direction = ">", levels = c("nein", "ja")))
y
calc.model.metrics.2 <- function(x.train, y.train, x.test, y.test, train.method = "glmnet", cv.method = "repeatedcv", number = 10, repeats = 5, metric = "ROC", tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",savePredictions = T,
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# obtain cv AUC of training folds
ci_cv <- ci.cv.AUC.lasso(md)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
predictions = data.frame(pred.ja = predict(md, x.test, type="prob")$ja, obs = y.test),
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
train.cv = data.frame(cvAUC = ci_cv$cvAUC,
se = ci_cv$se,
lower = ci_cv$ci[1],
upper = ci_cv$ci[2]),
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1], direction = ">", levels = c("nein", "ja"))), # control = 0, case = 1, direction: cases > controls
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred))
)
return(res)
}
