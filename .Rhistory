test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
1-test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.complete[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
out <- cvAUC(predictions, labels)
out
dat_compare
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
1-test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
cvAUC(predictions, labels)
library(cvAUC)
models.lasso.relaxedLasso
predictions
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
1-test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
cvAUC(predictions, labels)
dat_compare
ci.signif
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.signif[[d]], "[", "predictions")
lapply(1:10, function(x){
1-test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.signif[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
out <- cvAUC(predictions, labels)
out
ci.signif
feat.relaxed <-  feat.freq.complete[feat.freq.complete$freq > 0.5,]
feat.relaxed <- feat.relaxed[as.character(feat.relaxed$coef) %like any% names(dat_log),]
library(missForest)
library(tidyverse)
library(devtools)
library(caret)
library(doParallel)
library(pROC)
library(pbapply)
# source R functions
source_url("https://raw.githubusercontent.com/MBender1992/base_scripts/Marc/R_functions.R")
library(DescTools)
feat.relaxed <-  feat.freq.complete[feat.freq.complete$freq > 0.5,]
feat.relaxed <- feat.relaxed[as.character(feat.relaxed$coef) %like any% names(dat_log),]
models.lasso.relaxedLasso <- lassoEval("relaxedLasso", dat_log, rep = rep, k = k)
feat.relaxed <-  feat.freq.complete[feat.freq.complete$freq > 0.5,]
feat.relaxed <- feat.relaxed[as.character(feat.relaxed$coef) %like any% names(dat_log),]
# modelling and evaluation
models.lasso.relaxedLasso <- lassoEval("relaxedLasso", dat_log, rep = rep, k = k)
models.lasso.relaxedLasso <- setNames(lapply(models.lasso.relaxedLasso, setNames, folds), reps)
## confidence interval for the cv.train folds in the inner loop
ci.relaxedLasso <- rbind.model.ci(models.lasso.relaxedLasso)
# extract important coefficients
extract.coefs.relaxedLasso <- extractCoefs(models.lasso.relaxedLasso) %>% do.call(rbind,.) %>% table()
ci.relaxedLasso
feat.relaxed
feat.freq.complete
feat.relaxed
ci.miRNA
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.miRNA[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$pred.nein
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.miRNA[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
models.lasso.relaxedLasso
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
calc.model.metrics.2 <- function(x.train, y.train, x.test, y.test, train.method = "glmnet", cv.method = "repeatedcv", number = 10, repeats = 5, metric = "ROC", tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",savePredictions = T,
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# obtain cv AUC of training folds
ci_cv <- ci.cv.AUC.lasso(md)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
predictions = data.frame(pred.nein = predict(md, x.test, type="prob")$nein, obs = y.test),
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
train.cv = data.frame(cvAUC = ci_cv$cvAUC,
se = ci_cv$se,
lower = ci_cv$ci[1],
upper = ci_cv$ci[2]),
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1], direction = ">", levels = c("nein", "ja"))),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred))
)
return(res)
}
cvAUC(predictions, labels)
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.miRNA[[d]], "[", "predictions")
lapply(1:10, function(x){
1-test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.miRNA[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
out <- cvAUC(predictions, labels)
out
ci.relaxed.miRNA
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxed.miRNA[[d]], "[", "predictions")
lapply(1:10, function(x){
1-test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxed.miRNA[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
cvAUC(predictions, labels)
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
1-test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
out <- cvAUC(predictions, labels)
out
data.frame(cvAUC = out$fold.AUC, modelAUC = unlist.model(models.lasso.complete, "AUC", "test.metrics"))
out
data.frame(cvAUC = out$fold.AUC, modelAUC = unlist.model(models.lasso.relaxedLasso, "AUC", "test.metrics"))
pred <- ROCR::prediction(predictions[[95]],labels[[95]])
perf <- ROCR::performance(pred, "tpr", "fpr")
as.numeric(ROCR::performance(pred, measure = "auc",
x.measure = "cutoff")@y.values)
auc(roc(factor(labels[[95]], levels = c("nein", "ja")),predictions[[95]], direction = ">", levels = c("nein", "ja")))
auc(roc(labels[[95]],predictions[[95]], direction = ">", levels = c("nein", "ja")))
models.lasso.relaxedLasso
predictions[[95]]
labels[[95]]
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
1-test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
predictions
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
cvAUC(predictions, labels)
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
1-test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
performance(pred, measure = "auc",
x.measure = "cutoff")@y.values
as.numeric(ROCR::performance(pred, measure = "auc",
x.measure = "cutoff")@y.values)
pred
pred@predictions
predictions[[95]]
predictions[[95]]
labels[[95]]
labels[[95]]
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
labels[[95]]
as.numeric(ROCR::performance(pred, measure = "auc",
x.measure = "cutoff")
as.numeric(ROCR::performance(pred, measure = "auc",
x.measure = "cutoff")@y.values)
as.numeric(ROCR::performance(pred, measure = "auc",
x.measure = "cutoff")@y.values)
as.numeric(ROCR::performance(pred, measure = "sens",
x.measure = "cutoff")@y.values)
as.numeric(ROCR::performance(pred, measure = "sens",
x.measure = "cutoff")@y.values)
as.numeric(ROCR::performance(pred, measure = "sens",
x.measure = "cutoff")
as.numeric(ROCR::performance(pred, measure = "sens",
x.measure = "cutoff")@y.values)
performance(pred, measure = "sens",
x.measure = "cutoff")
ROCR::performance(pred, measure = "sens",
x.measure = "cutoff")@y.values
calc.model.metrics.2 <- function(x.train, y.train, x.test, y.test, train.method = "glmnet", cv.method = "repeatedcv", number = 10, repeats = 5, metric = "ROC", tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",savePredictions = T,
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# obtain cv AUC of training folds
ci_cv <- ci.cv.AUC.lasso(md)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
ROCR_pred <- ROCR::prediction(pred,y.test)
ROCR_perf <- ROCR::performance(ROCR_pred, "tpr", "fpr")
# object to return
res <- list(
predictions = data.frame(pred.nein = predict(md, x.test, type="prob")$nein, obs = y.test),
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
train.cv = data.frame(cvAUC = ci_cv$cvAUC,
se = ci_cv$se,
lower = ci_cv$ci[1],
upper = ci_cv$ci[2]),
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1], direction = ">", levels = c("nein", "ja"))),
AUC2 = as.numeric(ROCR::performance(ROCR_pred, measure = "auc", x.measure = "cutoff")@y.values),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred))
)
return(res)
}
models.lasso.relaxedLasso <- lassoEval("relaxedLasso", dat_log, rep = rep, k = k)
predictions[[95]]
labels[[95]]
md
pred <- predict(md, x.test, type="raw")
pred
pred <- predict(md, x.test, type="prob")
pred
ROCR_pred <- ROCR::prediction(pred,y.test)
ROCR_perf <- ROCR::performance(ROCR_pred, "tpr", "fpr")
x.test
y.test
pred <- predict(md, x.test, type="prob")
ROCR::prediction(pred,y.test)
y.test
pred
pred <- predict(md, x.test, type="prob")
ROCR::prediction(pred$ja,y.test)
ROCR_pred <- ROCR::prediction(pred$ja,y.test)
ROCR_perf <- ROCR::performance(ROCR_pred, "tpr", "fpr")
ROCR_pred
ROCR_perf
as.numeric(ROCR::performance(ROCR_pred, measure = "auc", x.measure = "cutoff")@y.values)
calc.model.metrics.2 <- function(x.train, y.train, x.test, y.test, train.method = "glmnet", cv.method = "repeatedcv", number = 10, repeats = 5, metric = "ROC", tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",savePredictions = T,
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# obtain cv AUC of training folds
ci_cv <- ci.cv.AUC.lasso(md)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="prob")
ROCR_pred <- ROCR::prediction(pred$ja,y.test)
ROCR_perf <- ROCR::performance(ROCR_pred, "tpr", "fpr")
# object to return
res <- list(
predictions = data.frame(pred.ja = pred$ja, obs = y.test),
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
train.cv = data.frame(cvAUC = ci_cv$cvAUC,
se = ci_cv$se,
lower = ci_cv$ci[1],
upper = ci_cv$ci[2]),
test.metrics = data.frame(AUC = auc(roc(y.test, pred[,1], direction = ">", levels = c("nein", "ja"))),
AUC2 = as.numeric(ROCR::performance(ROCR_pred, measure = "auc", x.measure = "cutoff")@y.values),
Sens = sensitivity(y.test, predict(md, x.test, type="raw"))  ,
Spec = specificity(y.test, predict(md, x.test, type="raw")))
)
return(res)
}
# modelling and evaluation
models.lasso.relaxedLasso <- lassoEval("relaxedLasso", dat_log, rep = rep, k = k)
#
rbind.model.ci <- function(model){
rbind(train.inner = data.frame(mean = mean(unlist.model(model, "cvAUC", "train.cv")),
lower = mean(unlist.model(model, "lower", "train.cv")),
upper = mean(unlist.model(model, "upper", "train.cv"))),
#train.outer = construct.ci(unlist.model(model, "ROC", "train.metrics")),
AUC2 = construct.ci(unlist.model(model, "AUC2", "test.metrics")),
test.outer =  construct.ci(unlist.model(model, "AUC", "test.metrics")))
}
# set names of list elements
models.lasso.relaxedLasso <- setNames(lapply(models.lasso.relaxedLasso, setNames, folds), reps)
## confidence interval for the cv.train folds in the inner loop
ci.relaxedLasso <- rbind.model.ci(models.lasso.relaxedLasso)
ci.relaxedLasso
#
rbind.model.ci <- function(model){
rbind(train.inner = data.frame(mean = mean(unlist.model(model, "cvAUC", "train.cv")),
lower = mean(unlist.model(model, "lower", "train.cv")),
upper = mean(unlist.model(model, "upper", "train.cv"))),
#train.outer = construct.ci(unlist.model(model, "ROC", "train.metrics")),
AUC2 = 1-construct.ci(unlist.model(model, "AUC2", "test.metrics")),
test.outer =  construct.ci(unlist.model(model, "AUC", "test.metrics")))
}
## confidence interval for the cv.train folds in the inner loop
ci.relaxedLasso <- rbind.model.ci(models.lasso.relaxedLasso)
ci.relaxedLasso
calc.model.metrics.2 <- function(x.train, y.train, x.test, y.test, train.method = "glmnet", cv.method = "repeatedcv", number = 10, repeats = 5, metric = "ROC", tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",savePredictions = T,
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# obtain cv AUC of training folds
ci_cv <- ci.cv.AUC.lasso(md)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="prob")
# object to return
res <- list(
predictions = data.frame(pred.ja = pred$ja, obs = y.test),
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
train.cv = data.frame(cvAUC = ci_cv$cvAUC,
se = ci_cv$se,
lower = ci_cv$ci[1],
upper = ci_cv$ci[2]),
test.metrics = data.frame(AUC = auc(roc(y.test, pred[,1], direction = ">", levels = c("nein", "ja"))),
Sens = sensitivity(y.test, predict(md, x.test, type="raw"))  ,
Spec = specificity(y.test, predict(md, x.test, type="raw")))
)
return(res)
}
#
rbind.model.ci <- function(model){
rbind(train.inner = data.frame(mean = mean(unlist.model(model, "cvAUC", "train.cv")),
lower = mean(unlist.model(model, "lower", "train.cv")),
upper = mean(unlist.model(model, "upper", "train.cv"))),
#train.outer = construct.ci(unlist.model(model, "ROC", "train.metrics")),
test.outer =  construct.ci(unlist.model(model, "AUC", "test.metrics")))
}
models.lasso.relaxedLasso <- lassoEval("relaxedLasso", dat_log, rep = rep, k = k)
# saveRDS(models.lasso.relaxedLasso, "models/models_lasso_relaxedLasso.rds")
# models.lasso.relaxedLasso <- readRDS("models/models_lasso_relaxedLasso.rds")
# set names of list elements
models.lasso.relaxedLasso <- setNames(lapply(models.lasso.relaxedLasso, setNames, folds), reps)
## confidence interval for the cv.train folds in the inner loop
ci.relaxedLasso <- rbind.model.ci(models.lasso.relaxedLasso)
# extract important coefficients
extract.coefs.relaxedLasso <- extractCoefs(models.lasso.relaxedLasso) %>% do.call(rbind,.) %>% table()
# calculate percentages
feat.freq <- data.frame(sort(extract.coefs.relaxedLasso/100)) %>%
setNames(c("coef", "freq"))
ci.relaxedLasso
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
1-test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
out <- cvAUC(predictions, labels)
out
plot(out$perf, col="grey82", lty=3, main="10-fold CV AUC")
#Plot CV AUC
plot(out$perf, col="red", avg="vertical", add=TRUE)
out
out$perf
plot(out$perf, col="grey82", lty=3, main="10-fold CV AUC", print.auc = T)
#Plot CV AUC
plot(out$perf, col="red", avg="vertical", add=TRUE, print.auc = T)
extractPredictions <- function(data){
lapply(1:10, function(x){
tmp <- sapply(data[[x]], '[', 'predictions')
bind_rows(tmp)
})
}
test <- extractPredictions(models.lasso.relaxedLasso.ROC)
roc.test <- bind_rows(test)
plot(roc(roc.test$obs, roc.test$pred.ja), print.auc = T)
out
ci.cvAUC(predictions, labels)
dat_compare
# combine data to compare models
dat_compare <- rbind(complete = ci.complete,
relaxedLasso = ci.relaxedLasso,
baseline = ci.baseline,
signif = ci.signif,
miRNA = ci.miRNA,
relaxedmiRNA = ci.relaxed.miRNA) %>%
rownames_to_column("tmp") %>%
separate(tmp,c("model", "results"), extra = "merge") %>%
mutate(model = factor(model),
model = reorder(model, mean))
dat_compare
ci.cvAUC
ci.cvAUC(predictions, labels)
ci.relaxed.miRNA
predictions <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
1-test[[x]]$pred.ja
})
})
predictions <- lapply(rapply(predictions, enquote, how="unlist"), eval)
labels <- lapply(1:10, function(d){
test <- sapply(models.lasso.relaxedLasso[[d]], "[", "predictions")
lapply(1:10, function(x){
test[[x]]$obs
})
})
labels <- lapply(rapply(labels, enquote, how="unlist"), eval)
predictions
labels
ci.cvAUC(predictions, labels)
abline(0,1)
