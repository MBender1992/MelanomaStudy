if(model == "complete"){
mm <- model.matrix(Responder~., data = data)[,-1]
} else if(model == "miRNA"){
mm <- model.matrix(Responder~., data = select(data, c(contains("mir"),Responder)))[,-1]
} else if(model == "baseline"){
mm <- model.matrix(Responder~., data = select(data, contains(c("Eosinophile","LDH","S100","CRP")),Responder))[,-1]
} else if(model == "signif"){
mm <- model.matrix(Responder~., data = select(data, contains(readRDS("significant_features.rds")),Responder))[,-1]
} else if(model == "relaxedLasso"){
mm <- model.matrix(Responder~., data = select(dat_log, c(feat.relaxed$coef,BRAF,Responder)))[,-1]
} else {
stop("Please specify 1 of the following 4 options:
1. 'baseline' for a base model using conventional serum markers (LDH, CRP, S100, Eosinophile)
2. 'miRNA' for a model using only miRNAs (reduced by lasso to informative features)
3. 'signif' for a model with significantly different features between responders and non-responders
4. 'complete' for a model with all predictors (reduced by lasso)
5. 'relaxedLasso' for a model with the best predictors selected by the 'complete' model (afterwards reduced again with LASSO)")
}
return(mm)
}
feat.relaxed <-  feat.freq.complete[feat.freq.complete$freq > 0.5,]
feat.relaxed <- feat.relaxed[feat.relaxed$coef != "BRAFpos",]
feat.relaxed
models.lasso.relaxedLasso <- readRDS("models/models_lasso_relaxedLasso.rds")
# set names of list elements
models.lasso.relaxedLasso <- setNames(lapply(models.lasso.relaxedLasso, setNames, folds), reps)
# extract metrics for inner fold (training) and outer fold (testing) from list and convert to df
df.train.relaxedLasso <- trainDF(models.lasso.relaxedLasso)
df.test.relaxedLasso <- testDF(models.lasso.relaxedLasso)
extract.coefs.relaxedLasso <- extractCoefs(models.lasso.relaxedLasso) %>% do.call(rbind,.) %>% table()
# calculate percentages
feat.freq <- data.frame(sort(extract.coefs.relaxedLasso/100)) %>%
setNames(c("coef", "freq"))
feat.freq
# plot important features
ggplot(data = feat.freq, aes(coef, freq)) +
geom_bar(stat = "identity",  color = "black", fill = "lightblue") +
coord_flip() +
xlab("") +
ylab("fraction of cv-models using this feature (relative feature importance)") +
theme_bw() +
scale_y_continuous(limits = c(0,1), breaks = seq(0,1,0.2), expand = c(0,0), labels = scales::percent_format()) +
geom_hline(yintercept = 0.5, lty = 2, color = "red")
feat.freq.complete <- data.frame(sort(extract.coefs.complete/100)) %>%
setNames(c("coef", "freq"))
# plot important features
ggplot(data = feat.freq.complete, aes(coef, freq, fill = ifelse(freq > 0.5, "red", "blue"))) +
geom_bar(stat = "identity",  color = "black") +
coord_flip() +
xlab("") +
ylab("fraction of cv-models using this feature (relative feature importance)") +
theme_bw() +
scale_y_continuous(limits = c(0,1), breaks = seq(0,1,0.2), expand = c(0,0), labels = scales::percent_format()) +
geom_hline(yintercept = 0.5, lty = 2, color = "red") +
scale_fill_manual(labels = c("< 50 %", "> 50 %"), values = c("gray95", "lightblue")) +
labs(fill = "frequency")
expand.grid(alpha = 1, lambda = seq(0.01,0.2,by = 0.01))
x <- model.matrix(Responder~., data = dat_log)[,-1]
x
y <- dat_log$Responder
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
library(missForest)
library(tidyverse)
library(devtools)
library(caret)
library(doParallel)
library(pROC)
library(pbapply)
# source R functions
source_url("https://raw.githubusercontent.com/MBender1992/base_scripts/Marc/R_functions.R")
# run glmnet model
md <- train(x, y, method = "glmnet",preProcess = c("center","scale"),
trControl = cctrl1,metric = "ROC",tuneGrid = expand.grid(alpha = 1, lambda = seq(0.01,0.2,by = 0.01)))
md$finalModel
md$pred
cctrl1 <- trainControl(method="repeatedcv", number=10,repeats = 5, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
md <- train(x, y, method = "glmnet",preProcess = c("center","scale"),
trControl = cctrl1,metric = "ROC",tuneGrid = expand.grid(alpha = 1, lambda = seq(0.01,0.2,by = 0.01)))
md
md$pred
md$results
md$pred
ci.cv_ROC <- function(data){
obs <-data$pred$obs
pred <- data$pred$pos
obs <-split(obs , f = data$pred$Resample)
pred <-split(pred , f = data$pred$Resample)
list(
cv_AUC = cvAUC(pred,obs),
ci.cvAUC = ci.cvAUC(pred,obs))
}
ci.cv_ROC(md)
md <- train(x, y, method = "glmnet",preProcess = c("center","scale"),
trControl = cctrl1,metric = "ROC",tuneGrid = expand.grid(alpha = 1, lambda = seq(0.01,0.2,by = 0.01)))
cctrl1 <- trainControl(method="repeatedcv", number=10,repeats = 5, returnResamp="all", savePredictions = T,
classProbs=TRUE, summaryFunction=twoClassSummary)
md <- train(x, y, method = "glmnet",preProcess = c("center","scale"),
trControl = cctrl1,metric = "ROC",tuneGrid = expand.grid(alpha = 1, lambda = seq(0.01,0.2,by = 0.01)))
md$pred
ci.cv_ROC <- function(md){
obs <-data$pred$obs
pred <- data$pred$pos
obs <-split(obs , f = data$pred$Resample)
pred <-split(pred , f = data$pred$Resample)
list(
cv_AUC = cvAUC(pred,obs),
ci.cvAUC = ci.cvAUC(pred,obs))
}
ci.cv_ROC
ci.cv_ROC <- function(data){
obs <-data$pred$obs
pred <- data$pred$pos
obs <-split(obs , f = data$pred$Resample)
pred <-split(pred , f = data$pred$Resample)
list(
cv_AUC = cvAUC(pred,obs),
ci.cvAUC = ci.cvAUC(pred,obs))
}
ci.cv_ROC(md)
md$pred$obs
ci.cv_ROC <- function(data){
obs <-data$pred$obs
pred <- data$pred$ja
obs <-split(obs , f = data$pred$Resample)
pred <-split(pred , f = data$pred$Resample)
list(
cv_AUC = cvAUC(pred,obs),
ci.cvAUC = ci.cvAUC(pred,obs))
}
ci.cv_ROC(md)
library(cvAUC)
ci.cv_ROC(md)
md
obs <-md$pred$obs
pred <- md$pred$ja
md$pred
md$pred$pred == md$pred$obs
md$pred$pred == md$pred$obs %>% mean(9)
md$pred$pred == md$pred$obs %>% mean()
md$pred$pred == md$pred$obs
class(md$pred$pred == md$pred$obs)
mean(md$pred$pred == md$pred$obs)
obs <-md$pred$obs
pred <- md$pred$ja
obs <-split(obs , f = md$pred$Resample)
obs
md$pred$Resample
pred <-split(pred , f = md$pred$Resample)
pred
cvAUC(pred,obs)
md$pred
md$pred
md$finalModel$lambda
md$finalModel$lambdaOpt
dat <- md %>% filter(lambda == md$finalModel$lambdaOpt)
md
dat <- md$pred %>% filter(lambda == md$finalModel$lambdaOpt)
obs <-dat$obs
pred <- dat$ja
obs <-split(obs , f = md$pred$Resample)
pred <-split(pred , f = md$pred$Resample)
obs
md$pred
md$pred %>% filter(lambda == md$finalModel$lambdaOpt)
dat <- md$pred %>% filter(lambda == md$finalModel$lambdaOpt)
obs <-dat$obs
obs
md$pred$obs
obs <-dat$obs
pred <- dat$ja
obs <-dat$obs
pred <- dat$ja
obs <-split(obs , f = dat$Resample)
pred <-split(pred , f = dat$Resample)
cvAUC(pred,obs)
dat
pred <- dat$nein
obs <-split(obs , f = dat$Resample)
pred <-split(pred , f = dat$Resample)
dat <- md$pred %>% filter(lambda == md$finalModel$lambdaOpt)
obs <-dat$obs
pred <- dat$nein
obs <-split(obs , f = dat$Resample)
pred <-split(pred , f = dat$Resample)
cvAUC(pred,obs)
ci.cv_ROC <- function(data){
obs <-data$pred$obs
pred <- data$pred$ja
obs <-split(obs , f = data$pred$Resample)
pred <-split(pred , f = data$pred$Resample)
list(
cv_AUC = cvAUC(pred,obs),
ci.cvAUC = ci.cvAUC(pred,obs))
}
dat <- md$pred %>% filter(lambda == md$finalModel$lambdaOpt)
ci.cv_ROC(dat)
dat
dat <- md$pred %>% filter(lambda == md$finalModel$lambdaOpt)
ci.cv_ROC <- function(data){
data <- filter(data, lambda == data$finalModel$lambdaOpt)
obs <-data$pred$obs
pred <- data$pred$ja
obs <-split(obs , f = data$pred$Resample)
pred <-split(pred , f = data$pred$Resample)
list(
cv_AUC = cvAUC(pred,obs),
ci.cvAUC = ci.cvAUC(pred,obs))
}
ci.cv_ROC <- function(data){
dat <- filter(data$pred, lambda == data$finalModel$lambdaOpt)
obs <-dat$obs
pred <- dat$ja
obs <-split(obs , f = dat$Resample)
pred <-split(pred , f = dat$pred$Resample)
list(
cv_AUC = cvAUC(pred,obs),
ci.cvAUC = ci.cvAUC(pred,obs))
}
ci.cv_ROC(md)
ci.cv_ROC <- function(data){
dat <- filter(data$pred, lambda == data$finalModel$lambdaOpt)
obs <-dat$obs
pred <- dat$ja
obs <-split(obs , f = dat$Resample)
pred <-split(pred , f = dat$Resample)
list(
cv_AUC = cvAUC(pred,obs),
ci.cvAUC = ci.cvAUC(pred,obs))
}
ci.cv_ROC(md)
ci.cv_ROC <- function(data){
dat <- filter(data$pred, lambda == data$finalModel$lambdaOpt)
obs <-dat$obs
pred <- dat$nein
obs <-split(obs , f = dat$Resample)
pred <-split(pred , f = dat$Resample)
list(
cv_AUC = cvAUC(pred,obs),
ci.cvAUC = ci.cvAUC(pred,obs))
}
ci.cv_ROC(md)
ci.cv.ROC.lasso <- function(data){
require(cvAUC)
dat <- filter(data$pred, lambda == data$finalModel$lambdaOpt)
obs <-dat$obs
pred <- dat$nein
obs <-split(obs , f = dat$Resample)
pred <-split(pred , f = dat$Resample)
list(
cv_AUC = cvAUC(pred,obs),
ci.cvAUC = ci.cvAUC(pred,obs))
}
ci.cv.ROC.lasso(md)
ci_cv <- ci.cv.ROC.lasso(md)
ci_cv$cv_AUC
ci.cv.ROC.lasso <- function(data){
require(cvAUC)
dat <- filter(data$pred, lambda == data$finalModel$lambdaOpt)
obs <-dat$obs
pred <- dat$nein
obs <-split(obs , f = dat$Resample)
pred <-split(pred , f = dat$Resample)
ci.cvAUC(pred,obs))
# list(
#   cv_AUC = cvAUC(pred,obs),
#   ci.cvAUC = ci.cvAUC(pred,obs))
}
ci.cv.ROC.lasso <- function(data){
require(cvAUC)
dat <- filter(data$pred, lambda == data$finalModel$lambdaOpt)
obs <-dat$obs
pred <- dat$nein
obs <-split(obs , f = dat$Resample)
pred <-split(pred , f = dat$Resample)
ci.cvAUC(pred,obs)
# list(
#   cv_AUC = cvAUC(pred,obs),
#   ci.cvAUC = ci.cvAUC(pred,obs))
}
ci.cv.ROC.lasso(md)
ci.cv.ROC.lasso <- function(data){
require(cvAUC)
dat <- filter(data$pred, lambda == data$finalModel$lambdaOpt)
obs <-dat$obs
pred <- dat$nein
obs <-split(obs , f = dat$Resample)
pred <-split(pred , f = dat$Resample)
ci.cvAUC(pred,obs)
}
# ...
ci_cv <- ci.cv.ROC.lasso(md)
ci_cv
data.frame(cvAUC = ci_cv$cv_AUC,
se = ci_cv$se,
lower = ci_cv$ci[1],
upper = ci_cv$ci[2])
ci_cv$cv_AUC
ci_cv <- ci.cv.ROC.lasso(md)
ci_cv$cv_AUC
data.frame(cvAUC = ci_cv$cvAUC,
se = ci_cv$se,
lower = ci_cv$ci[1],
upper = ci_cv$ci[2])
calc.model.metrics.2 <- function(x.train, y.train, x.test, y.test, train.method = "glmnet", cv.method = "repeatedcv", number = 10, repeats = 5, metric = "ROC", tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# obtain ci for cv
ci_cv <- ci.cv.ROC.lasso(md)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
train.cv = , data.frame(cvAUC = ci_cv$cvAUC,
se = ci_cv$se,
lower = ci_cv$ci[1],
upper = ci_cv$ci[2]),
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred))
)
return(res)
}
models.lasso.baseline <- lassoEval("baseline", dat_log, rep = rep, k = k)
saveRDS(models.lasso.baseline, "models/models_lasso_baseline.rds")
# models.lasso.baseline <- readRDS("models/models_lasso_baseline.rds")
# set names of list elements
models.lasso.baseline <- setNames(lapply(models.lasso.baseline, setNames, folds), reps)
# extract metrics for inner fold (training) and outer fold (testing) from list and convert to df
df.train.baseline <- trainDF(models.lasso.baseline)
df.test.baseline <- testDF(models.lasso.baseline)
# extract important coefficients
extract.coefs.baseline <- extractCoefs(models.lasso.baseline) %>% do.call(rbind,.) %>% table()
# calculate percentages
feat.freq <- data.frame(sort(extract.coefs.baseline/100)) %>%
setNames(c("coef", "freq"))
# plot important features
ggplot(data = feat.freq, aes(coef, freq, fill = ifelse(freq > 0.5, "red", "blue"))) +
geom_bar(stat = "identity",  color = "black") +
coord_flip() +
xlab("") +
ylab("fraction of cv-models using this feature (relative feature importance)") +
theme_bw() +
scale_y_continuous(limits = c(0,1), breaks = seq(0,1,0.2), expand = c(0,0), labels = scales::percent_format()) +
geom_hline(yintercept = 0.5, lty = 2, color = "red") +
scale_fill_manual(labels = c("< 50 %", "> 50 %"), values = c("gray95", "lightblue")) +
labs(fill = "frequency")
library(missForest)
library(tidyverse)
library(devtools)
library(caret)
library(doParallel)
library(pROC)
library(pbapply)
# source R functions
source_url("https://raw.githubusercontent.com/MBender1992/base_scripts/Marc/R_functions.R")
readRDS("models/models_lasso_complete.rds")
models.lasso.signif <- lassoEval("signif", dat_log, rep = rep, k = k)
saveRDS(models.lasso.signif, "models/models_lasso_signif.rds")
library(missForest)
library(tidyverse)
library(devtools)
library(caret)
library(doParallel)
library(pROC)
library(pbapply)
# source R functions
source_url("https://raw.githubusercontent.com/MBender1992/base_scripts/Marc/R_functions.R")
models.lasso.signif <- lassoEval("signif", dat_log, rep = rep, k = k)
saveRDS(models.lasso.signif, "models/models_lasso_signif.rds")
ci.cv.AUC.lasso <- function(data){
require(cvAUC)
dat <- filter(data$pred, lambda == data$finalModel$lambdaOpt)
obs <-dat$obs
pred <- dat$nein
obs <-split(obs , f = dat$Resample)
pred <-split(pred , f = dat$Resample)
ci.cvAUC(pred,obs)
}
calc.model.metrics.2 <- function(x.train, y.train, x.test, y.test, train.method = "glmnet", cv.method = "repeatedcv", number = 10, repeats = 5, metric = "ROC", tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",savePredictions = T,
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# obtain cv AUC of training folds
ci_cv <- ci.cv.AUC.lasso(md)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
train.cv = data.frame(cvAUC = ci_cv$cvAUC,
se = ci_cv$se,
lower = ci_cv$ci[1],
upper = ci_cv$ci[2]),
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred))
)
return(res)
}
# function to select which columns to use for the model matrix as specified by the model argument
model.matrix.subset <- function(model, data){
if(model == "complete"){
mm <- model.matrix(Responder~., data = data)[,-1]
} else if(model == "miRNA"){
mm <- model.matrix(Responder~., data = select(data, c(contains("mir"),Responder)))[,-1]
} else if(model == "baseline"){
mm <- model.matrix(Responder~., data = select(data, contains(c("Eosinophile","LDH","S100","CRP")),Responder))[,-1]
} else if(model == "signif"){
mm <- model.matrix(Responder~., data = select(data, contains(readRDS("significant_features.rds")),Responder))[,-1]
} else if(model == "relaxedLasso"){
mm <- model.matrix(Responder~., data = select(dat_log, c(feat.relaxed$coef,BRAF,Responder)))[,-1]
} else {
stop("Please specify 1 of the following 4 options:
1. 'baseline' for a base model using conventional serum markers (LDH, CRP, S100, Eosinophile)
2. 'miRNA' for a model using only miRNAs (reduced by lasso to informative features)
3. 'signif' for a model with significantly different features between responders and non-responders
4. 'complete' for a model with all predictors (reduced by lasso)
5. 'relaxedLasso' for a model with the best predictors selected by the 'complete' model (afterwards reduced again with LASSO)")
}
return(mm)
}
# models a function based on a presepcified model and evaluates training and test test using ROC, Sensitivity and Specificity
lassoEval <- function(model, dat, rep, k){
# define model matrix with selected features
x <- model.matrix.subset(model, data = dat)
# activate parallel computing
cl <- makeCluster(detectCores(), type='PSOCK')
registerDoParallel(cl)
# generate 10 folds for outer loop
set.seed(12)
fold.train <- createMultiFolds(y, k = k, times = rep) # ensure that at least 10 samples are in each fold
# split data based on these folds (Fold1 means that Fold1 is used for testing)
train.test.folds <- lapply(c(1:rep), function(split){
# select only folds containing the specified repeat in each iteration
if(split == 10){
ind <- names(fold.train) %>% str_detect("Rep10")
dat <- fold.train[ind]
} else {
ind <- names(fold.train) %>% str_detect(paste("Rep0",split, sep =""))
dat <- fold.train[ind]
}
# split data into training and test set with each fold being the test set once
res <- lapply(c(1:k), function(fold){
list(x.test = x[-dat[[fold]],],
x.train = x[dat[[fold]],],
y.test = y[-dat[[fold]]],
y.train = y[dat[[fold]]]
)
})
return(res)
})
# define name of the list elements
reps <<- paste0("Rep", 1:rep)
folds <<- paste0("Fold", 1:k)
train.test.folds <- setNames(lapply(train.test.folds, setNames, folds), reps)
set.seed(849)
lapply(c(1:rep), function(split){
# select Data from 1 repeat
dat <- train.test.folds[[paste("Rep",split, sep ="")]]
# print message to follow progress
message(paste("Starting calculation of Rep", split,"... of", rep))
# apply model to all folds of that 1 repeat and test against the remaining fold not used for training
res <- pblapply(c(1:k), function(fold){
calc.model.metrics.2(x.train = dat[[fold]]$x.train, y.train = dat[[fold]]$y.train, x.test =dat[[fold]]$x.test,
y.test = dat[[fold]]$y.test, train.method = "glmnet",
tuneGrid = expand.grid(alpha = 1, lambda = seq(0.01,0.2,by = 0.01)))
})
})
}
models.lasso.signif <- lassoEval("signif", dat_log, rep = rep, k = k)
saveRDS(models.lasso.signif, "models/models_lasso_signif.rds")
models.lasso.miRNA <- lassoEval("miRNA", dat_log, rep = 10, k = 10)
saveRDS(models.lasso.miRNA, "models/models_lasso_miRNA.rds")
models.lasso.miRNA <- setNames(lapply(models.lasso.miRNA, setNames, folds), reps)
# extract metrics for inner fold (training) and outer fold (testing) from list and convert to df
df.train.miRNA <- trainDF(models.lasso.miRNA)
df.test.miRNA <- testDF(models.lasso.miRNA)
# extract important coefficients
extract.coefs.miRNA <- extractCoefs(models.lasso.miRNA) %>% do.call(rbind,.) %>% table()
feat.freq <- data.frame(sort(extract.coefs.miRNA/100)) %>%
setNames(c("coef", "freq"))
# plot important features
ggplot(data = feat.freq, aes(coef, freq, fill = ifelse(freq > 0.5, "red", "blue"))) +
geom_bar(stat = "identity",  color = "black") +
coord_flip() +
xlab("") +
ylab("fraction of cv-models using this feature (relative feature importance)") +
theme_bw() +
scale_y_continuous(limits = c(0,1), breaks = seq(0,1,0.2), expand = c(0,0), labels = scales::percent_format()) +
geom_hline(yintercept = 0.5, lty = 2, color = "red") +
scale_fill_manual(labels = c("< 50 %", "> 50 %"), values = c("gray95", "lightblue")) +
labs(fill = "frequency")
