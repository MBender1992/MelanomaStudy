})
calc.model.metrics.2
lapply(1:5, function(split){
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'test.metrics'))
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'test.metrics'))
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'test.metrics'))
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'test.metrics'))
models.test[[split]], '[', 'test.metrics')) %>%
summarize(mean = mean(AUC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
df.test <- lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'test.metrics')) #%>%
#summarize(mean = mean(AUC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
df.test
train.test.folds
df.train
df.test <- lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'test.metrics')) #%>%
#summarize(mean = mean(AUC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
df.test
df.test <- lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'test.metrics')) %>%
summarize(mean = mean(AUC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
df.test
df.train
models.test
lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'coefficients'))# %>%
#summarize(mean = mean(ROC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
models.test$Rep1
models.eNet <- models.test
models.lasso <-lapply(c(1:rep), function(split){
# select Data from 1 repeat
dat <- train.test.folds[[paste("Rep",split, sep ="")]]
message(paste("Starting calculation of Rep", split,"... of", rep))
# apply model to all folds of that 1 repeat and test against the remaining fold not used for training
res <- pblapply(c(1:k), function(fold){
calc.model.metrics.2(dat[[fold]]$x.train, y = dat[[fold]]$y.train, train.method = "glmnet",
tuneGrid = expand.grid(alpha = 1, lambda = seq(0.01,0.2,by = 0.01)))
})
return(res)
})
models.lasso
models.test <- models.lasso
models.test <- setNames(lapply(models.test, setNames, folds), reps)
df.train <- lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'train.metrics')) %>%
summarize(mean = mean(ROC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
df.train
df.test <- lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'test.metrics')) %>%
summarize(mean = mean(AUC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
df.test
models.test <- models.eNet
test_class_cv_model
set.seed(849)
test_class_cv_model <- train(x, dat3$Responder, method = "glmnet",
trControl = cctrl1,metric = "ROC",tuneGrid = expand.grid(alpha = seq(0,1,0.1),
lambda = seq(0.001,0.2,by = 0.001)))
library(missForest)
library(tidyverse)
library(devtools)
library(caret)
library(doParallel)
library(pROC)
library(pbapply)
set.seed(849)
test_class_cv_model <- train(x, dat3$Responder, method = "glmnet",
trControl = cctrl1,metric = "ROC",tuneGrid = expand.grid(alpha = seq(0,1,0.1),
lambda = seq(0.001,0.2,by = 0.001)))
x
x <- model.matrix(Responder~., data = dat_log)[,-1]
y <- dat_log$Responder
x
set.seed(849)
test_class_cv_model <- train(x, y, method = "glmnet",
trControl = cctrl1,metric = "ROC",tuneGrid = expand.grid(alpha = seq(0,1,0.1),
lambda = seq(0.001,0.2,by = 0.001)))
cctrl1 <- trainControl(method="cv", number=10, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
y
set.seed(849)
test_class_cv_model <- train(x, y, method = "glmnet",
trControl = cctrl1,metric = "ROC",tuneGrid = expand.grid(alpha = seq(0,1,0.1),
lambda = seq(0.001,0.2,by = 0.001)))
test_class_cv_model
test_class_cv_model$finalModel
predict(test_class_cv_model, x)
pred <-predict(test_class_cv_model, x)
obs <- y
roc_obj <- roc(obs, pred$ja)
pred <-predict(test_class_cv_model, x, type = "prob")
obs <- y
roc_obj <- roc(obs, pred$ja)
auc(roc_obj)
obs
pred$ja
test_class_cv_model$pred
# Using caret to perform CV
cctrl1 <- trainControl(method="cv", number=10, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary, savePredictions = T)
set.seed(849)
test_class_cv_model <- train(x, y, method = "glmnet",
trControl = cctrl1,metric = "ROC",tuneGrid = expand.grid(alpha = seq(0,1,0.1),
lambda = seq(0.001,0.2,by = 0.001)))
test_class_cv_model
test_class_cv_model$pred
pred <-predict(test_class_cv_model, x, type = "prob")
obs <- y
roc_obj <- roc(obs, pred$ja)
auc(roc_obj)
test_class_cv_model
test_class_cv_model$pred %>% filter(alpha == 0.9 & lambda == 0.007)
selectIndices <- test_class_cv_model$pred %>% filter(alpha == 0.9 & lambda == 0.007)
test_class_cv_model$pred$M
test_class_cv_model$pred
plot.roc(test_class_cv_model$pred$obs[selectedIndices],
test_class_cv_model$pred$ja[selectedIndices])
selectIndices <- test_class_cv_model$pred %>% filter(alpha == 0.9 & lambda == 0.007)
selectedIndices <- test_class_cv_model$pred %>% filter(alpha == 0.9 & lambda == 0.007)
plot.roc(test_class_cv_model$pred$obs[selectedIndices],
test_class_cv_model$pred$ja[selectedIndices])
selectedIndices
selectedIndices <- test_class_cv_model$pred$alpha == 0.9 & test_class_cv_model$pred$lambda == 0.007
plot.roc(test_class_cv_model$pred$obs[selectedIndices],
test_class_cv_model$pred$ja[selectedIndices])
?plot.roc
plot.roc(test_class_cv_model$pred$obs[selectedIndices],
test_class_cv_model$pred$ja[selectedIndices],print.auc =T)
iris <- iris[iris$Species == "virginica" | iris$Species == "versicolor", ]
iris$Species <- factor(iris$Species)  # setosa should be removed from factor
samples <- sample(NROW(iris), NROW(iris) * .5)
data.train <- iris[samples, ]
data.test <- iris[-samples, ]
forest.model <- train(Species ~., data.train)
result.predicted.prob <- predict(forest.model, data.test, type="prob") # Prediction
result.predicted.prob
result.roc <- roc(data.test$Species, result.predicted.prob$versicolor) # Draw ROC curve.
plot(result.roc, print.thres="best", print.thres.best.method="closest.topleft")
forest.model
plot(result.roc, print.thres="best",print.auc = T, print.thres.best.method="closest.topleft")
forest.model <- train(Species ~., data.train, metric = "ROC")
forest.model <- train(Species ~., data.train, metric = "ROC",trControl = cctrl1)
forest.model
forest.model
result.predicted.prob <- predict(forest.model, data.test, type="prob") # Prediction
result.roc <- roc(data.test$Species, result.predicted.prob$versicolor) # Draw ROC curve.
plot(result.roc, print.thres="best",print.auc = T, print.thres.best.method="closest.topleft")
# split data based on these folds (Fold1 means that Fold1 is used for testing)
train.test.folds
models.eNet
lasso.model
test_class_cv_model
test_class_cv_model$finalModel
test_class_cv_model$finalModel$tuneValue
test_class_cv_model$finalModel
predict(test_class_cv_model)
pred <- predict(test_class_cv_model)
obs <- y
pred <- predict(test_class_cv_model, type = "prob")
obs <- y
roc_obj <- roc(obs, pred$ja)
auc(roc_obj)
fold.train <- createDataPartition(y,p = 0.8)
fold.train
x.train <- x[fold.train,]
fold.train
fold.train <- createDataPartition(y,p = 0.8, times = 1)
fold.train
x.train <- x[fold.train$Resample1,]
x.train <- x[fold.train$Resample1,]
x.test <- x[-fold.train$Resample1,]
y.train <- x[fold.train$Resample1]
y.test <- x[-fold.train$Resample1]
set.seed(849)
test_class_cv_model <- train(x.train, y.train, method = "glmnet",
trControl = cctrl1,metric = "ROC",tuneGrid = expand.grid(alpha = seq(0,1,0.1),
lambda = seq(0.001,0.2,by = 0.001)))
# Using caret to perform CV
cctrl1 <- trainControl(method="cv", number=10, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary, savePredictions = T)
set.seed(849)
test_class_cv_model <- train(x.train, y.train, method = "glmnet",
trControl = cctrl1,metric = "ROC",tuneGrid = expand.grid(alpha = seq(0,1,0.1),
lambda = seq(0.001,0.2,by = 0.001)))
# Using caret to perform CV
cctrl1 <- trainControl(method="cv", number=10, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
set.seed(849)
test_class_cv_model <- train(x.train, y.train, method = "glmnet",
trControl = cctrl1,metric = "ROC",tuneGrid = expand.grid(alpha = seq(0,1,0.1),
lambda = seq(0.001,0.2,by = 0.001)))
x.train
set.seed(849)
test_class_cv_model <- train(x.train, y.train, method = "glmnet",
trControl = cctrl1,metric = "ROC",tuneGrid = expand.grid(alpha = seq(0,1,0.1),
lambda = seq(0.001,0.2,by = 0.001)))
x.train
y.train
View(dat_serum_markers_tidy)
x.train <- x[fold.train$Resample1,]
x.test <- x[-fold.train$Resample1,]
y.train <- y[fold.train$Resample1]
y.test <- y[-fold.train$Resample1]
set.seed(849)
test_class_cv_model <- train(x.train, y.train, method = "glmnet",
trControl = cctrl1,metric = "ROC",tuneGrid = expand.grid(alpha = seq(0,1,0.1),
lambda = seq(0.001,0.2,by = 0.001)))
pred <- predict(test_class_cv_model, x.test, type = "prob")
pred
obs <- y.test
roc_obj <- roc(obs, pred$ja)
auc(roc_obj)
set.seed(13)
fold.train <- createDataPartition(y,p = 0.9, times = 1)
x.train <- x[fold.train$Resample1,]
x.test <- x[-fold.train$Resample1,]
y.train <- y[fold.train$Resample1]
y.test <- y[-fold.train$Resample1]
set.seed(849)
test_class_cv_model <- train(x.train, y.train, method = "glmnet",
trControl = cctrl1,metric = "ROC",tuneGrid = expand.grid(alpha = seq(0,1,0.1),
lambda = seq(0.001,0.2,by = 0.001)))
test_class_cv_model
test_class_cv_model$finalModel
test_class_cv_model$results
test_class_cv_model$results
test_class_cv_model$results$ROC %>% max()
pred <- predict(test_class_cv_model, x.test, type = "prob")
obs <- y.test
roc_obj <- roc(obs, pred$ja)
auc(roc_obj)
predict(test_class_cv_model, type = "prob")
obs <- x.test
roc_obj <- roc(obs, pred$ja)
auc(roc_obj)
predict(test_class_cv_model, type = "prob")
obs <- x.test
roc(obs, pred$ja)
pred
pred <- predict(test_class_cv_model, type = "prob")
obs <- x.test
roc_obj <- roc(obs, pred$ja)
pred
obs <- x.test
obs
set.seed(13)
fold.train <- createDataPartition(y,p = 0.9, times = 1)
x.train <- x[fold.train$Resample1,]
x.test <- x[-fold.train$Resample1,]
y.train <- y[fold.train$Resample1]
y.test <- y[-fold.train$Resample1]
obs <- y.train
roc_obj <- roc(obs, pred$ja)
auc(roc_obj)
set.seed(27)
fold.train <- createDataPartition(y,p = 0.9, times = 1)
x.train <- x[fold.train$Resample1,]
x.test <- x[-fold.train$Resample1,]
y.train <- y[fold.train$Resample1]
y.test <- y[-fold.train$Resample1]
set.seed(849)
test_class_cv_model <- train(x.train, y.train, method = "glmnet",
trControl = cctrl1,metric = "ROC",tuneGrid = expand.grid(alpha = seq(0,1,0.1),
lambda = seq(0.001,0.2,by = 0.001)))
test_class_cv_model$results$ROC %>% max()
pred <- predict(test_class_cv_model, x.test, type = "prob")
obs <- y.test
roc_obj <- roc(obs, pred$ja)
auc(roc_obj)
pred <- predict(test_class_cv_model, type = "prob")
obs <- y.train
roc_obj <- roc(obs, pred$ja)
auc(roc_obj)
set.seed(123)
fold.train <- createDataPartition(y,p = 0.9, times = 1)
x.train <- x[fold.train$Resample1,]
x.test <- x[-fold.train$Resample1,]
y.train <- y[fold.train$Resample1]
y.test <- y[-fold.train$Resample1]
x.test
y.test
set.seed(3)
fold.train <- createDataPartition(y,p = 0.9, times = 1)
x.train <- x[fold.train$Resample1,]
x.test <- x[-fold.train$Resample1,]
y.train <- y[fold.train$Resample1]
y.test <- y[-fold.train$Resample1]
y.test
set.seed(849)
test_class_cv_model <- train(x.train, y.train, method = "glmnet",
trControl = cctrl1,metric = "ROC",tuneGrid = expand.grid(alpha = seq(0,1,0.1),
lambda = seq(0.001,0.2,by = 0.001)))
test_class_cv_model$results$ROC %>% max()
pred <- predict(test_class_cv_model, x.test, type = "prob")
obs <- y.test
roc_obj <- roc(obs, pred$ja)
auc(roc_obj)
pred <- predict(test_class_cv_model, type = "prob")
obs <- y.train
roc_obj <- roc(obs, pred$ja)
auc(roc_obj)
cl <- makeCluster(detectCores(), type='PSOCK')
registerDoParallel(cl)
# generate 5 folds for outer loop
rep <- 5
k <- 10
set.seed(12)
fold.train <- createMultiFolds(y, k = k, times = rep) # ensure that at least 10 samples are in each fold
# split data based on these folds (Fold1 means that Fold1 is used for testing)
train.test.folds <- lapply(c(1:rep), function(split){
ind <- names(fold.train) %>% str_detect(paste("Rep",split, sep =""))
dat <- fold.train[ind]
res <- lapply(c(1:k), function(fold){
list(x.test = x[-dat[[fold]],],
x.train = x[dat[[fold]],],
y.test = y[-dat[[fold]]],
y.train = y[dat[[fold]]]
)
return(res)
})
})
reps <- paste0("Rep", 1:rep)
folds <- paste0("Fold", 1:k)
train.test.folds <- setNames(lapply(train.test.folds, setNames, folds), reps)
models.lasso <-lapply(c(1:rep), function(split){
# select Data from 1 repeat
dat <- train.test.folds[[paste("Rep",split, sep ="")]]
message(paste("Starting calculation of Rep", split,"... of", rep))
# apply model to all folds of that 1 repeat and test against the remaining fold not used for training
res <- pblapply(c(1:k), function(fold){
set.seed(849)
calc.model.metrics.2(dat[[fold]]$x.train, y = dat[[fold]]$y.train, train.method = "glmnet",
tuneGrid = expand.grid(alpha = 1, lambda = seq(0.01,0.2,by = 0.01)))
})
return(res)
})
x <- model.matrix(Responder~., data = dat_log)[,-1]
y <- dat_log$Responder
registerDoParallel(cl)
# generate 5 folds for outer loop
rep <- 5
k <- 10
set.seed(12)
fold.train <- createMultiFolds(y, k = k, times = rep) # ensure that at least 10 samples are in each fold
# split data based on these folds (Fold1 means that Fold1 is used for testing)
train.test.folds <- lapply(c(1:rep), function(split){
ind <- names(fold.train) %>% str_detect(paste("Rep",split, sep =""))
dat <- fold.train[ind]
res <- lapply(c(1:k), function(fold){
list(x.test = x[-dat[[fold]],],
x.train = x[dat[[fold]],],
y.test = y[-dat[[fold]]],
y.train = y[dat[[fold]]]
)
return(res)
})
})
reps <- paste0("Rep", 1:rep)
folds <- paste0("Fold", 1:k)
train.test.folds <- setNames(lapply(train.test.folds, setNames, folds), reps)
calc.model.metrics.2 <- function(x.train, y.train,train.method = "glmnet", cv.method = "repeatedcv", number = 10, repeats = 5, metric = "ROC", tuneGrid){
# define ctrl function
cctrl1 <- trainControl(method=cv.method, number=number,repeats = repeats, returnResamp="all",
classProbs=TRUE, summaryFunction=twoClassSummary)
# run glmnet model
md <- train(x.train, y.train, method = train.method,preProcess = c("center","scale"),
trControl = cctrl1,metric = metric,tuneGrid = tuneGrid)
# train coefs
feat <- coef(md$finalModel, md$finalModel$lambdaOpt)
# obtain index from max metric
opt <- md$results[which(md$results$lambda == md$finalModel$lambdaOpt),]
# predict
pred <- predict(md, x.test, type="raw")
# object to return
res <- list(
coefficients = rownames_to_column(data.frame(vals = feat[feat[,1] != 0, 1][-1]),"coefs"),
train.metrics = opt[which(opt$ROC == max(opt$ROC)),],
test.metrics = data.frame(AUC = auc(roc(y.test, predict(md, x.test, type="prob")[,1])),
Sens = sensitivity(y.test, pred)  ,
Spec = specificity(y.test, pred))
)
return(res)
}
models.lasso <-lapply(c(1:rep), function(split){
# select Data from 1 repeat
dat <- train.test.folds[[paste("Rep",split, sep ="")]]
message(paste("Starting calculation of Rep", split,"... of", rep))
# apply model to all folds of that 1 repeat and test against the remaining fold not used for training
res <- pblapply(c(1:k), function(fold){
set.seed(849)
calc.model.metrics.2(dat[[fold]]$x.train, y = dat[[fold]]$y.train, train.method = "glmnet",
tuneGrid = expand.grid(alpha = 1, lambda = seq(0.01,0.2,by = 0.01)))
})
return(res)
})
models.lasso <-lapply(c(1:rep), function(split){
# select Data from 1 repeat
dat <- train.test.folds[[paste("Rep",split, sep ="")]]
message(paste("Starting calculation of Rep", split,"... of", rep))
# apply model to all folds of that 1 repeat and test against the remaining fold not used for training
res <- pblapply(c(1:k), function(fold){
calc.model.metrics.2(dat[[fold]]$x.train, y = dat[[fold]]$y.train, train.method = "glmnet",
tuneGrid = expand.grid(alpha = 1, lambda = seq(0.01,0.2,by = 0.01)))
})
return(res)
})
train.test.folds
train.test.folds
train.test.folds <- lapply(c(1:rep), function(split){
ind <- names(fold.train) %>% str_detect(paste("Rep",split, sep =""))
dat <- fold.train[ind]
res <- lapply(c(1:k), function(fold){
list(x.test = x[-dat[[fold]],],
x.train = x[dat[[fold]],],
y.test = y[-dat[[fold]]],
y.train = y[dat[[fold]]]
)
return(res)
})
})
fold.train
train.test.folds <- lapply(c(1:rep), function(split){
ind <- names(fold.train) %>% str_detect(paste("Rep",split, sep =""))
dat <- fold.train[ind]
res <- lapply(c(1:k), function(fold){
list(x.test = x[-dat[[fold]],],
x.train = x[dat[[fold]],],
y.test = y[-dat[[fold]]],
y.train = y[dat[[fold]]]
)
return(res)
})
})
train.test.folds
ind
dat
fold.train[ind]
dat <- fold.train[ind]
train.test.folds <- lapply(c(1:rep), function(split){
ind <- names(fold.train) %>% str_detect(paste("Rep",split, sep =""))
dat <- fold.train[ind]
res <- lapply(c(1:k), function(fold){
list(x.test = x[-dat[[fold]],],
x.train = x[dat[[fold]],],
y.test = y[-dat[[fold]]],
y.train = y[dat[[fold]]]
)
return(res)
})
})
train.test.folds
c(1:rep)
x
x[-dat[[1]],]
train.test.folds <- lapply(c(1:rep), function(split){
ind <- names(fold.train) %>% str_detect(paste("Rep",split, sep =""))
dat <- fold.train[ind]
res <- lapply(c(1:k), function(fold){
list(x.test = x[-dat[[fold]],],
x.train = x[dat[[fold]],],
y.test = y[-dat[[fold]]],
y.train = y[dat[[fold]]]
)
})
return(res)
})
train.test.folds
reps <- paste0("Rep", 1:rep)
folds <- paste0("Fold", 1:k)
train.test.folds <- setNames(lapply(train.test.folds, setNames, folds), reps)
set.seed(849)
models.lasso <-lapply(c(1:rep), function(split){
# select Data from 1 repeat
dat <- train.test.folds[[paste("Rep",split, sep ="")]]
message(paste("Starting calculation of Rep", split,"... of", rep))
# apply model to all folds of that 1 repeat and test against the remaining fold not used for training
res <- pblapply(c(1:k), function(fold){
calc.model.metrics.2(dat[[fold]]$x.train, y = dat[[fold]]$y.train, train.method = "glmnet",
tuneGrid = expand.grid(alpha = 1, lambda = seq(0.01,0.2,by = 0.01)))
})
return(res)
})
models.lasso
models.test <- models.lasso
# set names of list elements
models.test <- setNames(lapply(models.test, setNames, folds), reps)
df.train <- lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'train.metrics')) %>%
summarize(mean = mean(ROC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
df.train <-
df.train
df.train
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'train.metrics')) %>%
summarize(mean = mean(ROC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
df.train <- lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'train.metrics')) %>%
summarize(mean = mean(ROC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
df.train
df.test <- lapply(1:rep, function(split){
do.call(rbind.data.frame, sapply(models.test[[split]], '[', 'test.metrics')) %>%
summarize(mean = mean(AUC), meanSens = mean(Sens, na.rm=T), meanSpec = mean(Spec))
})
df.test
